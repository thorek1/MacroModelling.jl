
using MacroModelling
import Turing: NUTS, HMC, PG, IS, sample, logpdf, Truncated#, Normal, Beta, Gamma, InverseGamma,
using CSV, DataFrames, AxisKeys
import Zygote
import ForwardDiff
import ChainRulesCore: @ignore_derivatives, ignore_derivatives, rrule, NoTangent, @thunk
using Random
import BenchmarkTools: @benchmark
Random.seed!(1)
# ]add CSV, DataFrames, Zygote, AxisKeys, MCMCChains, Turing, DynamicPPL, Pigeons, StatsPlots
println("Threads used: ", Threads.nthreads())

smpler = "nuts" #
mdl = "linear" # 
fltr = :kalman
algo = :first_order

sample_idx = 47:230
# sample_idx = 47:47
dat = CSV.read("benchmark/usmodel.csv", DataFrame)

# Initialize a DataFrame to store the data
df = DataFrame(iteration = Float64[])

if mdl == "linear"
    include("../models/Smets_Wouters_2007_linear.jl")
    Smets_Wouters_2007 = Smets_Wouters_2007_linear
elseif mdl == "nonlinear"
    include("../models/Smets_Wouters_2007.jl")
end


# load data
data = KeyedArray(Array(dat)',Variable = Symbol.(strip.(names(dat))), Time = 1:size(dat)[1])

# declare observables
observables_old = [:dy, :dc, :dinve, :labobs, :pinfobs, :dw, :robs] # note that :dw was renamed to :dwobs in linear model in order to avoid confusion with nonlinear model

# Subsample
# subset observables in data
data = data(observables_old, sample_idx)

observables = [:dy, :dc, :dinve, :labobs, :pinfobs, :dwobs, :robs] # note that :dw was renamed to :dwobs in linear model in order to avoid confusion with nonlinear model

data = rekey(data, :Variable => observables)

SS(Smets_Wouters_2007, parameters = [:crhoms => 0.01, :crhopinf => 0.01, :crhow => 0.01,:cmap => 0.01,:cmaw => 0.01], algorithm = algo)(observables)

ğ“‚ = Smets_Wouters_2007


parameters = [  0.5295766584252728
0.25401999781328677
0.5555813987579575
0.3654903601830364
0.2294564856713931
0.12294028349908431
0.20767050150368016
0.9674674841230338
0.20993223738088435
0.9888169549988175
0.8669340301385475
0.07818383624087137
0.6105112778170307
0.37671694996404337
0.2187231627543815
0.1362385298510586
6.3886101979474015
1.6678696241559958
0.6799655079831786
0.9424292929726574
2.502826072472096
0.6570767721691694
0.6729083298930368
0.23408903978575385
0.6457362272648652
1.4738116352107862
2.088069269612668
0.8655409607264644
0.0895375194503755
0.18792207697672325
0.696046453737325
0.1899464169442222
-0.5748023731804703
0.3683194328119635
0.5101771887138438
0.17425592648706756]


z_ea, z_eb, z_eg, z_eqs, z_em, z_epinf, z_ew, crhoa, crhob, crhog, crhoqs, crhoms, crhopinf, crhow, cmap, cmaw, csadjcost, csigma, chabb, cprobw, csigl, cprobp, cindw, cindp, czcap, cfc, crpi, crr, cry, crdy, constepinf, constebeta, constelab, ctrend, cgy, calfa = parameters

ctou, clandaw, cg, curvp, curvw = Smets_Wouters_2007.parameter_values[indexin([:ctou,:clandaw,:cg,:curvp,:curvw],Smets_Wouters_2007.parameters)]

parameters_combined = [ctou, clandaw, cg, curvp, curvw, calfa, csigma, cfc, cgy, csadjcost, chabb, cprobw, csigl, cprobp, cindw, cindp, czcap, crpi, crr, cry, crdy, crhoa, crhob, crhog, crhoqs, crhoms, crhopinf, crhow, cmap, cmaw, constelab, constepinf, constebeta, ctrend, z_ea, z_eb, z_eg, z_em, z_ew, z_eqs, z_epinf]

get_loglikelihood(ğ“‚, data, parameters_combined, verbose = false, presample_periods = 4, filter = fltr, algorithm = algo, initial_covariance = :diagonal) # -1082.8088568705207
#old order -1087.2659101980191


import Zygote
import ForwardDiff
import FiniteDifferences

back_grad = Zygote.gradient(x -> get_loglikelihood(ğ“‚, data, x, verbose = false, presample_periods = 4, filter = fltr, algorithm = algo, initial_covariance = :diagonal), parameters_combined)[1]

fini_grad = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> get_loglikelihood(ğ“‚, data, x, verbose = false, presample_periods = 4, filter = fltr, algorithm = algo, initial_covariance = :diagonal), parameters_combined)[1]

fini_grad â‰ˆ back_grad

forw_grad = ForwardDiff.gradient(x -> get_loglikelihood(ğ“‚, data, x, verbose = false, presample_periods = 4, filter = fltr, algorithm = algo, initial_covariance = :diagonal), parameters_combined)

forw_grad â‰ˆ back_grad
forw_grad â‰ˆ fini_grad

@benchmark Zygote.gradient(x -> get_loglikelihood(ğ“‚, data, x, verbose = false, presample_periods = 4, filter = fltr, algorithm = algo, initial_covariance = :diagonal), parameters_combined)[1]

@profview for i in 1:100 Zygote.gradient(x -> get_loglikelihood(ğ“‚, data, x, verbose = false, presample_periods = 4, filter = fltr, algorithm = algo, initial_covariance = :diagonal), parameters_combined)[1] end

import LinearAlgebra: mul!, transpose!, rmul!, logdet
import LinearAlgebra as â„’
import ChainRulesCore: @ignore_derivatives, ignore_derivatives
import MacroModelling: get_and_check_observables, solve!, check_bounds, get_relevant_steady_state_and_state_update, calculate_loglikelihood, get_initial_covariance
parameter_values = parameters_combined
algorithm = :first_order
filter = :kalman
warmup_iterations = 0
presample_periods = 0
initial_covariance = :diagonal
tol = 1e-12
verbose = false
T = ğ“‚.timings


    observables = @ignore_derivatives get_and_check_observables(ğ“‚, data)

    @ignore_derivatives solve!(ğ“‚, verbose = verbose, algorithm = algorithm)

    bounds_violated = @ignore_derivatives check_bounds(parameter_values, ğ“‚)

    NSSS_labels = @ignore_derivatives [sort(union(ğ“‚.exo_present, ğ“‚.var))..., ğ“‚.calibration_equations_parameters...]

    obs_indices = @ignore_derivatives convert(Vector{Int}, indexin(observables, NSSS_labels))

    TT, SS_and_pars, ğ’, state, solved = get_relevant_steady_state_and_state_update(Val(algorithm), parameter_values, ğ“‚, tol)

    # prepare data
    data_in_deviations = collect(data(observables)) .- SS_and_pars[obs_indices]

    observables_index = @ignore_derivatives convert(Vector{Int},indexin(observables,sort(union(T.aux,T.var,T.exo_present))))

    observables_and_states = @ignore_derivatives sort(union(T.past_not_future_and_mixed_idx,observables_index))

    A = ğ’[observables_and_states,1:T.nPast_not_future_and_mixed] * â„’.diagm(ones( length(observables_and_states)))[@ignore_derivatives(indexin(T.past_not_future_and_mixed_idx,observables_and_states)),:]
    B = ğ’[observables_and_states,T.nPast_not_future_and_mixed+1:end]

    C = â„’.diagm(ones(length(observables_and_states)))[@ignore_derivatives(indexin(sort(observables_index), observables_and_states)),:]

    ğ = B * B'

    # Gaussian Prior
    coordinates = @ignore_derivatives Tuple{Vector{Int}, Vector{Int}}[]
    
    dimensions = @ignore_derivatives [size(A),size(ğ)]
    
    values = vcat(vec(A), vec(collect(-ğ)))


    ####### old ########
    P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)

    u = zeros(size(C,2))

    z = C * u

    loglik = (0.0)

    for t in 1:size(data_in_deviations, 2)
        v = data_in_deviations[:, t] - z

        F = C * P * C'

        luF = â„’.lu(F, check = false) ###

        if !â„’.issuccess(luF)
            return -Inf
        end

        Fdet = â„’.det(luF)

        if Fdet < eps(Float64)
            return -Inf
        end

        invF = inv(luF) ###

        if t > presample_periods
            loglik += log(Fdet) + v' * invF * v###
        end

        K = P * C' * invF

        P = A * (P - K * C * P) * A' + ğ

        u = A * (u + K * v)

        z = C * u
    end


    -(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 




    #### more explicit


    v = data_in_deviations[:, t] - C * u

    if t > presample_periods
        llh = loglik + logdet(C * P * C') + v' * inv(C * P * C') * v###
    end

    uÌ‚ = A * (u + P * C' * inv(C * P * C') * v)

    PÌ‚ = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ




    ######## new
    P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)
    observables = data_in_deviations

    T = size(observables, 2) + 1

    u = [zeros(size(C,2)) for _ in 1:T]

    u_mid = deepcopy(u)

    z = [zeros(size(observables, 1)) for _ in 1:T]

    P_mid = [deepcopy(P) for _ in 1:T]

    temp_N_N = similar(P)

    P = deepcopy(P_mid)

    B_prod = ğ
    # Ct = collect(C')
    CP = [zero(C) for _ in 1:T]

    K = [zero(C') for _ in 1:T]

    cc = C * C'

    V = [zero(cc) for _ in 1:T]

    invV = [zero(cc) for _ in 1:T]

    V[1] += â„’.I
    invV[1] = inv(V[1])

    innovation = deepcopy(z)

    # V[1] .= C * P[1] * C'

    loglik = (0.0)

    for t in 2:T
        # Kalman iteration
        mul!(CP[t], C, P_mid[t-1]) # CP[t] = C * P[t]

        # V[t] = CP[t] * C' + R
        mul!(V[t], CP[t], C')
        # V[t].mat .+= R

        luV = â„’.lu(V[t], check = false)
        Vdet = â„’.det(luV)
        if Vdet < eps(Float64)
            return -Inf
        end
        invV[t] .= inv(luV)
        
        innovation[t] .= observables[:, t-1] - z[t-1]
        # loglik += logpdf(MvNormal(V[t]), innovation[t])  # no allocations since V[t] is a PDMat
        if t - 1 > presample_periods
            loglik += log(Vdet) + innovation[t]' * invV[t] * innovation[t]
        end

        # K[t] .= CP[t]' / V[t]  # Kalman gain
        mul!(K[t], P_mid[t-1] * C', invV[t])

        #u[t] += K[t] * innovation[t]
        copy!(u[t], u_mid[t-1])
        mul!(u[t], K[t], innovation[t], 1, 1)

        #P[t] -= K[t] * CP[t]
        copy!(P[t], P_mid[t-1])
        mul!(P[t], K[t], CP[t], -1, 1)

        # this was moved down indicating a timing difference between the two approaches
        mul!(u_mid[t], A, u[t]) # u[t] = A u[t-1]
        mul!(z[t], C, u_mid[t]) # z[t] = C u[t]

        # P[t] = A * P[t - 1] * A' + B * B'
        mul!(temp_N_N, P[t], A')
        mul!(P_mid[t], A, temp_N_N)
        P_mid[t] .+= B_prod
    end

    -(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 



    for t in 2:T
        # Kalman iteration
        u[t] = A * u[t-1]
        z[t] = C * u[t]

        P[t] = A * P[t - 1] * A' + B * B'

        CP[t] = C * P[t]

        V[t] = CP[t] * C' + R

        innovation[t] .= observables[:, t-1] - z[t-1]

        # loglik += logpdf(MvNormal(V[t]), innovation[t])  # no allocations since V[t] is a PDMat
        if t - 1 > presample_periods
            loglik += log(Vdet) + innovation[t]' * invV[t] * innovation[t]
        end

        K[t] .= CP[t]' / V[t]  # Kalman gain

        u[t] += K[t] * innovation[t]
        P[t] -= K[t] * CP[t]

    end

    for t in T:-1:2
        # pullback
        # Sensitivity accumulation
        # P[t] -= K[t] * CP[t]
        copy!(Î”P_mid, Î”P)
        Î”K = -Î”P * CP[t]'
        Î”CP = - K[t]' * Î”P

        # u[t] += K[t] * innovation[t]
        copy!(Î”u_mid, Î”u)
        Î”K += Î”u * innovation[t]'
        Î”innovation += -K[t]'* Î”u

        # K[t] .= CP[t]' / V[t]
        Î”CP += invV[t] * Î”K'
        Î”V .= -invV[t] * CP[t] * Î”K * invV[t]

        # loglik += log(Vdet) + innovation[t]' * invV[t] * innovation[t]
        Î”innovation += 2 * Î”logpdf * invV[t] * innovation[t] # Î£^-1 * (z_obs - z)
        Î”V -= Î”logpdf * (invV[t] - invV[t] * innovation[t] * innovation[t]' * invV[t])

        # innovation[t] .= observables[:, t-1] - z[t-1]
        Î”observables = Î”innovation
        Î”z = -Î”innovation

        # V[t] = CP[t] * C' + R
        Î”C += Î”V * C * P_mid[t]' + Î”V' * C * P_mid[t]
        Î”P_mid += C' * Î”V * C

        # CP[t] = C * P[t]
        Î”C += Î”CP * P_mid[t]'
        Î”P_mid += C' * Î”CP

        # P[t] = A * P[t - 1] * A' + B
        Î”A += Î”P_mid * A * P[t - 1]
        Î”P .= A' * Î”P_mid * A # pass into next period
        Î”B += Î”P_mid

        # z[t] = C * u[t]
        Î”C += Î”z * u_mid[t]'
        Î”u_mid += C' * Î”z

        # u[t] = A * u[t-1]
        Î”A += Î”u_mid * u[t - 1]'
        Î”u += A' * Î”u_mid
    end



    ### check derivatives fro llh
    
    function calc_llh(c, p)
        # innovation[t]' * invV[t] * innovation[t]
        f = c * p * c'
        return -logdet(f)/2
    end

    function rrule(::typeof(calc_llh), c, p)
        f = c * p * c'

        function pullback(âˆ‚llh)
            âˆ‚P = -c' * inv(f) * c * âˆ‚llh/2
            return NoTangent(), NoTangent(), âˆ‚P
        end
        return -logdet(f)/2, pullback
    end

    calc_llh(C, P[1])
    grad1 = Zygote.gradient(x -> calc_llh(C, x), P[1])[1]#|>sparse

    grad2 = Zygote.gradient(x -> -logdet(C * x * C')/2, P[1])[1]#|>sparse
    
    grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> -logdet(C * x * C')/2, P[1])[1]


    isapprox(grad3, grad1)
    isapprox(grad3, grad2)
    ### derivatives for second part of llh
        
    function calc_llh2(P, C, v)
        -v' * inv(C * P * C') * v / 2
    end

    function rrule(::typeof(calc_llh2), P, C, v)
        invF = inv(C * P * C')
        
        function pullback(âˆ‚llh)
            âˆ‚F = -invF * (v * v') * invF * âˆ‚llh
            âˆ‚P = C' * âˆ‚F * C
            return NoTangent(), âˆ‚P, NoTangent(), NoTangent()
        end

        return -v' * invF * v / 2, pullback
    end


    calc_llh2(P[1], C, v)
    C' * v * v' * C 
    grad2 = Zygote.gradient(x -> v' * inv(C * x * C') * v, P[1])[1]#|>sparse

    grad1 = Zygote.gradient(x -> calc_llh2(x, C, v), P[1])[1]#|>sparse

    grad1 == grad2

    # derivative for u and data_in_deviations
    v = data_in_deviations[:, t] - C * u
    
    llh = loglik + logdet(C * P * C') + v' * inv(C * P * C') * v###



    function calc_llh3(P, C, u, data)
        -(data - C * u)' * inv(C * P * C') * (data - C * u)/2
    end

    function rrule(::typeof(calc_llh3), P, C, u, data)
        invF = inv(C * P * C')
        
        function pullback(âˆ‚llh)
            âˆ‚PÌ‚ = -invF * (v * v') * invF * âˆ‚llh
            âˆ‚P = C' * âˆ‚PÌ‚ * C
            âˆ‚u = -2 * C' * invF * (data - C * u) * âˆ‚llh
            âˆ‚data = 2 * invF * (data - C * u) * âˆ‚llh
            return NoTangent(), âˆ‚P, NoTangent(), âˆ‚u, âˆ‚data
        end

        return - (data - C * u)' * inv(C * P * C') * (data - C * u) / 2, pullback
    end


    calc_llh3(P[1], C, u[1], data_in_deviations[:,t])

    grad2 = Zygote.gradient(x -> (data_in_deviations[:, t] - C * x)' * inv(C * P[1] * C') * (data_in_deviations[:, t] - C * x), u[1])[1]#|>sparse


    grad1 = Zygote.gradient(x -> calc_llh3(P[1], C, x, data_in_deviations[:,t]), u[1])[1]#|>sparse
    


    grad2 == grad1
    
    


    grad2 = Zygote.gradient(x -> (x - C * u[1])' * inv(C * P[1] * C') * (x - C * u[1]), data_in_deviations[:, t])[1]#|>sparse

    grad1 = Zygote.gradient(x -> calc_llh3(P[1], C, u[1], x), data_in_deviations[:,t])[1]#|>sparse
    
    grad2 == grad1
    
    





    function calc_llh4(P, C, u, data)
        F = C * P * C'
        v = data - C * u
        return -(logdet(F) + v' * inv(F) * v) / 2
    end

    function rrule(::typeof(calc_llh4), P, C, u, data)
        F = C * P * C'
        invF = inv(F)
        v = data - C * u

        function pullback(âˆ‚llh)
            âˆ‚PÌ‚ = invF * (v * v') * invF * âˆ‚llh/2
            âˆ‚P = C' * âˆ‚PÌ‚ * C - C' * invF * C * âˆ‚llh/2
            âˆ‚u = C' * invF * (data - C * u) * âˆ‚llh
            âˆ‚data = -invF * (data - C * u) * âˆ‚llh
            return NoTangent(), âˆ‚P, NoTangent(), âˆ‚u, âˆ‚data
        end

        return -(logdet(F) + v' * invF * v) / 2, pullback
    end


    
    calc_llh4(P[1], C, u[1], data_in_deviations[:,t])

    grad2 = Zygote.gradient(x -> -(logdet(C * x * C') + (data_in_deviations[:, t] - C * u[1])' * inv(C * x * C') * (data_in_deviations[:, t] - C * u[1])) / 2 , P[1])[1]#|>sparse

    grad1 = Zygote.gradient(x -> calc_llh4(x, C, u[1], data_in_deviations[:,t]), P[1])[1]#|>sparse
    
    grad2 == grad1


# P
grad1 = Zygote.gradient(x -> calc_llh4(x, C, u[1], data_in_deviations[:,t]), P[1])[1]#|>sparse
    
grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> calc_llh4(x, C, u[1], data_in_deviations[:,t]), P[1])[1]


isapprox(grad3, grad1, rtol = 1e-7)

# u
grad1 = Zygote.gradient(x -> calc_llh4(P[1], C, x, data_in_deviations[:,t]), u[1])[1]#|>sparse
    
grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> calc_llh4(P[1], C, x, data_in_deviations[:,t]), u[1])[1]


isapprox(grad3, grad1, rtol = 1e-7)



# data
grad1 = Zygote.gradient(x -> calc_llh4(P[1], C, u[1], x), data_in_deviations[:,t])[1]#|>sparse
    
grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> calc_llh4(P[1], C, u[1], x), data_in_deviations[:,t])[1]

isapprox(grad3, grad1, rtol = 1e-7)


    


# now moving to the next time period



if t > presample_periods
    llh = loglik + logdet(C * P * C') + v' * inv(C * P * C') * v###
end

uÌ‚ = A * (u + P * C' * inv(C * P * C') * v)

PÌ‚ = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ

v = data_in_deviations[:, t] - C * uÌ‚

llh = loglik + logdet(C * PÌ‚ * C') + v' * inv(C * PÌ‚ * C') * v



logdet(C * (A * (P[1] - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C')

grad2 = Zygote.gradient(x -> logdet(C * (A * (x - x * C' * inv(C * x * C') * C * x) * A' + ğ) * C'), P[1])[1]



Zygote.gradient(x -> logdet(C * (A * (P - x * C' * invF * C * x) * A' + ğ) * C'), P[1])




function logdet_transform(p, C, A, ğ)
    logdet(C * (A * (p - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C')
end

logdet_transformed(P[1], C, A, ğ)



function rrule(::typeof(logdet_transformed), P, C, A, B, invF)
    # Step 1: Compute intermediate matrices
    M = P - P * C' * invF * C * P
    N = A * M * A' + ğ
    Omega = C * N * C'

    # Step 2: Compute function value
    logdet_value = logdet(Omega)
    
    # Step 3: Define the pullback
    function logdet_transformed_pullback(Î”logdet)
        # Computing gradients using chain rule as derived
        Omega_inv = inv(Omega)
        dN_dM = A' * (C' * Omega_inv * C) * A
        # Gradient of M wrt x involves the derivative of a quadratic form
        dM_dx = â„’.I - 2 * C' * invF * C * P

        # Pullback calculation
        âˆ‚x = dM_dx * dN_dM * Î”logdet
        return NoTangent(), âˆ‚x, NoTangent(), NoTangent(), NoTangent(), NoTangent()
    end

    return logdet_value, logdet_transformed_pullback
end


using ChainRulesCore

function rrule(::typeof(logdet_transform), P, C, A, B)
    # Intermediate matrix definitions
    Y = C * P * C'
    Y_inv = inv(Y)
    inner_matrix = P - P * C' * Y_inv * C * P
    transformed_matrix = A * inner_matrix * A' + B
    Omega = C * transformed_matrix * C'
    
    # Compute logdet
    logdet_val = logdet(Omega)

    # Define the pullback
    function logdet_custom_pullback(Î”logdet)
        print("yes")
        # Compute gradients based on chain rule
        Omega_inv = inv(Omega)
        dTransformed_dInner = A' * (C' * Omega_inv * C) * A
        dInner_dx = â„’.I - 2 * C' * Y_inv * C * P
        âˆ‚x = dInner_dx * dTransformed_dInner * Î”logdet
        
        return NoTangent(), âˆ‚x, NoTangent(), NoTangent(), NoTangent(), NoTangent()
    end

    return logdet_val, logdet_custom_pullback
end






gba = Zygote.gradient(x -> logdet(C * (A * (x - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C'), P[1])[1]

gfo = ForwardDiff.gradient(x -> logdet_transform(x, C, A, ğ), P[1])

gba2 =  Zygote.gradient(x -> logdet_transform(x, C, A, ğ), P[1])[1]

FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> logdet_transform(x, C, A, ğ), P[1])[1]

gfi = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> logdet(C * (A * (x - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C'), P[1])[1]


isapprox(gfi, gba)
isapprox(gfi, gba2)
isapprox(gfi, gfo)






âˆ‚P = -C' * inv(C * P * C') * C * âˆ‚llh/2

-âˆ‚P / âˆ‚llh * 2
Y = C * P * C'
Y_inv = inv(C * P * C')
inner_matrix = P - P * C' * Y_inv * C * P
transformed_matrix = A * inner_matrix * A' + B
Omega = C * (A * (P - P * C' * inv(C * P * C') * C * P) * A' + B) * C'


Omega_inv = inv(Omega)
dTransformed_dInner = A' * (C' * Omega_inv * C) * A
# dInner_dx = â„’.I - 2 * C' * Y_inv * C * P
dInner_dx = â„’.I + 2 / âˆ‚llh * 2 * âˆ‚P * P
âˆ‚x = -âˆ‚llh / 2 * dInner_dx * dTransformed_dInner

dInner_dx = â„’.I + 2 / âˆ‚llh * 2 * âˆ‚P * P
âˆ‚x = -(â„’.I * âˆ‚llh / 2 + 2 * âˆ‚P * P) * dTransformed_dInner




Omega_inv = inv(C * (A * (P - P * C' * inv(C * P * C') * C * P) * A' + B) * C')
dTransformed_dInner = A' * (C' * Omega_inv * C) * A
dInner_dx = â„’.I - 2 * C' * inv(C * P * C') * C * P
âˆ‚x = dInner_dx * dTransformed_dInner * Î”logdet



# now go for the second part


function vv_transform(p, C, v)
    v' * inv(C * (A * (p - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C') * v
end

vv_transform(P[1], C, v)

gba = Zygote.gradient(x -> logdet(C * (A * (x - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C'), P[1])[1]

gfo = ForwardDiff.gradient(x -> logdet_transform(x, C, A, ğ), P[1])

gba2 =  Zygote.gradient(x -> logdet_transform(x, C, A, ğ), P[1])[1]

FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> logdet_transform(x, C, A, ğ), P[1])[1]

gfi = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x -> logdet(C * (A * (x - P[1] * C' * inv(C * P[1] * C') * C * P[1]) * A' + ğ) * C'), P[1])[1]


isapprox(gfi, gba)
isapprox(gfi, gba2)
isapprox(gfi, gfo)



U = deepcopy(u[1])

# and now trying for th complete part

function calc_llh_complete(x, C, A, ğ, data_in_deviations)
    llh = 0.0
    u = zeros(size(A,2))
    P = x



    for data in eachcol(data_in_deviations)
        llh += logdet(C * P * C') + (data - C * u)' * inv(C * P * C') * (data - C * u)###

        u = A * (u + P * C' * inv(C * P * C') * (data - C * u))

        P = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ
    end



    return llh
end

for data in eachcol(data_in_deviations)
    println(data)
end

calc_llh_complete(P[1], C, A, ğ, data_in_deviations)


function rrule(::typeof(calc_llh_complete), x, C, A, ğ, data_in_deviations)
    function likelihood_pullback(Î”llh)
        llh = 0.0
        u = zeros(size(A, 2))
        P = x
        âˆ‚P = zero(P)  # Initialize the gradient for P

        # Backward pass must also accumulate changes for gradient calculation
        for data in eachcol(data_in_deviations)
            v = data - C * u
            CPC = C * P * C'
            CPC_inv = inv(CPC)
            llh_contrib = logdet(CPC) + v' * CPC_inv * v
            llh += llh_contrib

            # Gradient calculations for each step
            G = P * C' * CPC_inv
            âˆ‚llh_contrib = Î”llh  # From upstream, affect on llh
            âˆ‚P += âˆ‚llh_contrib * G' * C' * CPC_inv * v  # Chain rule application
            âˆ‚P += âˆ‚llh_contrib * CPC_inv  # from logdet derivative

            u = A * (u + G * v)
            P = A * (P - G * C * P) * A' + ğ
        end

        return NoTangent(), âˆ‚P, NoTangent(), NoTangent(), NoTangent(), NoTangent()
    end

    llh = 0.0  # Initialization as in the original function
    # Repeat the forward computation to get the final llh
    u = zeros(size(A, 2))
    P = x
    for data in eachcol(data_in_deviations)
        v = data - C * u
        CPC = C * P * C'
        llh += logdet(CPC) + v' * inv(CPC) * v
        u = A * (u + P * C' * inv(CPC) * v)
        P = A * (P - P * C' * inv(CPC) * C * P) * A' + ğ
    end
    return llh, likelihood_pullback
end

gradzyg = Zygote.gradient(x -> calc_llh_complete(x, C, A, ğ, data_in_deviations), P[1])[1]




using ChainRulesCore

# Define the function whose rrule we are setting up
function kalman_update(C, A, B, data_in_deviations, x)
    llh = 0.0
    u = zeros(size(A, 2))
    P = x
    stored_values = []

    for t in 1:size(data_in_deviations, 2)
        v = data_in_deviations[:, t] - C * u
        CPC = C * P * C'
        invCPC = inv(CPC)
        llh += logdet(CPC) + v' * invCPC * v

        # Store values needed for the backward pass
        push!(stored_values, (copy(u), copy(P), copy(v), copy(CPC), copy(invCPC)))

        u = A * (u + P * C' * invCPC * v)
        P = A * (P - P * C' * invCPC * C * P) * A' + B
    end

    return llh, stored_values
end

# Define the rrule
function rrule(::typeof(kalman_update), C, A, B, data_in_deviations, x)
    llh = 0.0
    u = zeros(size(A, 2))
    P = x

    # First iteration
    v1 = data_in_deviations[:, 1] - C * u
    CPC1 = C * P * C'
    invCPC1 = inv(CPC1)
    llh += logdet(CPC1) + v1' * invCPC1 * v1
    u = A * (u + x * C' * inv(C * x * C') * v1)
    P2 = A * (P - P * C' * invCPC1 * C * P) * A' + B

    # Second iteration
    v2 = data_in_deviations[:, 2] - C * u
    CPC2 = C * P2 * C'
    invCPC2 = inv(CPC2)
    llh += logdet(CPC2) + v2' * invCPC2 * v2

    function compute_llh_pullback(Î”llh)
        # Initialize gradient accumulations
        âˆ‚P = zero(x)
        # âˆ‚u = zero(size(A, 2))

        # Backward for the second iteration
        # âˆ‚v2 = 2 * invCPC2 * v2 * Î”llh
        âˆ‚CPC2 = invCPC2' * Î”llh - invCPC2' * (v2 * v2') * invCPC2' * Î”llh
        âˆ‚P += C' * âˆ‚CPC2 * C# + A' * (C' * âˆ‚CPC2 * C) * A
        âˆ‚P += A' * (âˆ‚P) * A
        # Backward for the first iteration
        # âˆ‚v1 = 2 * invCPC1 * v1 * Î”llh
        âˆ‚CPC1 = invCPC1' * Î”llh - invCPC1' * (v1 * v1') * invCPC1' * Î”llh
        âˆ‚P += C' * âˆ‚CPC1 * C  # Since P was transformed by A
        # âˆ‚P += A' * âˆ‚P * A
        # âˆ‚P +=   # Since P was transformed by A
        # println("yks")

        return NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), âˆ‚P
    end

    return llh, compute_llh_pullback
end

# PP * A' 

kalman_update(C, A, ğ, data_in_deviations[:,1], P[1])
A * C'

gradfin = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> kalman_update(C, A, ğ, data_in_deviations[:,1:2], x)[1], PP)[1]


gradzyg = Zygote.gradient(x -> kalman_update(C, A, ğ, data_in_deviations[:,1:2], x)[1], PP)[1]
gradfor = ForwardDiff.gradient(x -> kalman_update(C, A, ğ, data_in_deviations[:,1:2], x)[1], PP)

isapprox(gradfin, gradzyg)
isapprox(gradfin, gradfor)
isapprox(gradzyg, gradfor)

gradfin - gradfor

gradfin - gradzyg

v = C * u[1]
CPC = C * P[1] * C'
CPC_inv = inv(CPC)
# llh_contrib = logdet(CPC) + v' * CPC_inv * v
# llh += llh_contrib

# Gradient calculations for each step
G = P[1] * C' * CPC_inv
# âˆ‚llh_contrib = Î”llh  # From upstream, affect on llh
âˆ‚P += âˆ‚llh_contrib * G' * C' * CPC_inv * v  # Chain rule application


PP = get_initial_covariance(Val(:theoretical), values, coordinates, dimensions)


gradfin = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), x->begin
llh = 0.0
u = zeros(size(A,2))
P = x

v = data_in_deviations[:, 1] - C * u

llh += logdet(C * P * C') + v' * inv(C * P * C') * v

u = A * (u + x * C' * inv(C * x * C') * v)

P = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ

v = data_in_deviations[:, 2] - C * u

llh += logdet(C * P * C') + v' * inv(C * P * C') * v

return llh
end, PP)[1]

gradfin2 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> kalman_update(C, A, ğ, data_in_deviations[:,1:2], x)[1], PP)[1]


isapprox(gradfin, gradfin2)



gradfin = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> calc_llh_complete(x, C, A, ğ, data_in_deviations), P[1])[1]





grad2 = Zygote.gradient(x->begin
llh = 0.0
u = zeros(size(A,2))
P = x

v = data_in_deviations[:, 1] - C * u

llh += logdet(C * P * C') + v' * inv(C * P * C') * v

u = A * (u + x * C' * inv(C * x * C') * v)

P = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ

v = data_in_deviations[:, 2] - C * u

llh += logdet(C * P * C') + v' * inv(C * P * C') * v

return llh
end, P[1])[1]



grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> begin
            llh = 0.0
            u = zeros(size(A,2))
            P = x
            for i in 1:10#size(data_in_deviations,2)
                v = data_in_deviations[:, t] - C * u

                llh += logdet(C * P * C') + v' * inv(C * P * C') * v###

                u = A * (u + P * C' * inv(C * P * C') * v)

                P = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ
        end
        return llh
    end, PP)[1]

isapprox(grad3, grad2)


grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
x -> begin
            llh = 0.0
            P = x
            for i in 1:10#size(data_in_deviations,2)
                llh += logdet(C * P * C') 

                P = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ
        end
        return llh
    end, PP)[1]


grad2 = Zygote.gradient(x -> begin
llh = 0.0
P = x
for i in 1:10#size(data_in_deviations,2)
    llh += logdet(C * P * C') 

    P = A * (P - P * C' * inv(C * P * C') * C * P) * A' + ğ
end
return llh
end, PP)[1]#|>sparse

isapprox(grad3, grad2)




using ChainRulesCore

function compute_llh_recursive(C, A, B, x)
    llh = 0.0
    P = x
    # Ps = [P]  # To store each iteration of P for the backward pass

    for i in 1:10#size(data_in_deviations, 2)
        CPC = C * P * C'
        llh += logdet(CPC)

        invCPC = inv(CPC)
        P = A * (P - P * C' * invCPC * C * P) * A' + B
        # push!(Ps, P)  # Store updated P
    end
    return llh
end

compute_llh_recursive(C, A, ğ, P[1])

function rrule(::typeof(compute_llh_recursive), C, A, B, x)
    llh = 0.0
    P = x
    Ps = [P]  # To store each iteration of P for the backward pass

    for i in 1:10
        CPC = C * P * C'
        llh += logdet(CPC)

        invCPC = inv(CPC)
        P = A * (P - P * C' * invCPC * C * P) * A' + B
        push!(Ps, P)  # Store updated P
    end

    function compute_llh_recursive_pullback(Î”llh)
        âˆ‚P = zero(P)  # Initialize gradient for P as zero
        
        # Back-propagate through each iteration
        for i in reverse(1:10)
            P = Ps[i]  # Retrieve P from the corresponding forward iteration
            CPC = C * P * C'
            invCPC = inv(CPC)

            # Gradient of logdet contributes to P
            âˆ‚CPC = invCPC' * Î”llh  # Gradient of logdet wrt CPC
            âˆ‚P_current = C' * âˆ‚CPC * C  # Chain rule to get âˆ‚P_current

            # Backprop through the update formula for P
            if i > 1  # For all but the first iteration where P isn't dependent on previous Ps
                P_prev = Ps[i]
                # âˆ‚P_next = A' * (âˆ‚P - (P_prev * C' * invCPC * C * âˆ‚P + âˆ‚P * C' * invCPC * C * P_prev) + âˆ‚P * A
                âˆ‚P_next = A' * (âˆ‚P - ((P_prev * C' * invCPC * C * âˆ‚P) + (âˆ‚P * C' * invCPC * C * P_prev))) * A
            
                âˆ‚P = âˆ‚P_next
            end

            # Add current iteration's contribution to âˆ‚P
            âˆ‚P += âˆ‚P_current
        end

        return NoTangent(), NoTangent(), NoTangent(), NoTangent(), âˆ‚P  # Gradients for C, A, B are NO_FIELDS as they are constants
    end

    return llh, compute_llh_recursive_pullback
end



grad2 = Zygote.gradient(x -> compute_llh_recursive(C, A, ğ, x),PP)[1]

grad3 = FiniteDifferences.grad(FiniteDifferences.central_fdm(3,1), 
                                x -> compute_llh_recursive(C, A, ğ, x),PP)[1]



grad2 = Zygote.gradient(x -> v' * inv(C * x * C') * v, P[1])[1]#|>sparse


    grad1 = Zygote.gradient(x -> calc_llh2(x, C, v), P[1])[1]#|>sparse




    grad2 == grad1
    
    
    
    Zygote.gradient(logdet, F)[1]#|>sparse

    C' * inv(F) * C
C' * 10 * C

C' * inv(F) * C * Î”logdet

    ####### new but old order

    P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)
    observables = data_in_deviations

    T = size(observables, 2) + 1

    u = [zeros(size(C,2)) for _ in 1:T]

    u_mid = deepcopy(u)

    z = [zeros(size(observables, 1)) for _ in 1:T]

    P_mid = [deepcopy(P) for _ in 1:T]

    temp_N_N = similar(P)

    P = deepcopy(P_mid)

    B_prod = ğ
    # Ct = collect(C')
    CP = [zero(C) for _ in 1:T]

    K = [zero(C') for _ in 1:T]

    cc = C * C'

    V = [zero(cc) for _ in 1:T]

    invV = [zero(cc) for _ in 1:T]

    V[1] += â„’.I
    invV[1] = inv(V[1])

    innovation = deepcopy(z)

    # V[1] .= C * P[1] * C'

    loglik = (0.0)

    for t in 2:T
        # Kalman iteration
        # this was moved down indicating a timing difference between the two approaches
        mul!(u_mid[t], A, u[t-1]) # u[t] = A u[t-1]
        mul!(z[t], C, u_mid[t]) # z[t] = C u[t]

        # P[t] = A * P[t - 1] * A' + B * B'
        mul!(temp_N_N, P[t-1], A')
        mul!(P_mid[t], A, temp_N_N)
        P_mid[t] .+= B_prod

        mul!(CP[t], C, P_mid[t]) # CP[t] = C * P[t]

        # V[t] = CP[t] * C' + R
        mul!(V[t], CP[t], C')
        # V[t].mat .+= R

        luV = â„’.lu(V[t], check = false)
        Vdet = â„’.det(luV)
        if Vdet < eps(Float64)
            return -Inf
        end
        invV[t] .= inv(luV)
        
        innovation[t] .= observables[:, t-1] - z[t]
        # loglik += logpdf(MvNormal(V[t]), innovation[t])  # no allocations since V[t] is a PDMat
        if t - 1 > presample_periods
            loglik += log(Vdet) + innovation[t]' * invV[t] * innovation[t]
        end

        # K[t] .= CP[t]' / V[t]  # Kalman gain
        mul!(K[t], P_mid[t] * C', invV[t])

        #u[t] += K[t] * innovation[t]
        copy!(u[t], u_mid[t])
        mul!(u[t], K[t], innovation[t], 1, 1)

        #P[t] -= K[t] * CP[t]
        copy!(P[t], P_mid[t])
        mul!(P[t], K[t], CP[t], -1, 1)
    end

    -(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 





    # reverse pass new but old order
    Î”logpdf = 1.0
    temp_L_N = similar(C)
    temp_N_L = similar(C')
    temp_L_L = similar(V[1])
    temp_M = similar(z[1])

    # Buffers
    Î”P = zero(P[1])
    Î”u = zero(u[1])
    Î”A = zero(A)
    Î”B = zero(B)
    Î”C = zero(C)
    Î”K = zero(K[1])
    Î”P_mid = zero(Î”P)
    Î”P_mid_sum = zero(Î”P)
    Î”CP = zero(CP[1])
    Î”u_mid = zero(u_mid[1])
    Î”z = zero(z[1])
    Î”V = zero(V[1])

    for t in T:-1:2+presample_periods
        # Sensitivity accumulation
        copy!(Î”P_mid, Î”P)
        mul!(Î”K, Î”P, CP[t]', -1, 0) # i.e. Î”K = -Î”P * CP[t]'
        mul!(Î”CP, K[t]', Î”P, -1, 0) # i.e. Î”CP = - K[t]' * Î”P
        copy!(Î”u_mid, Î”u)
        mul!(Î”K, Î”u, innovation[t]', 1, 1) # Î”K += Î”u * innovation[t]'
        mul!(Î”z, K[t]', Î”u, -1, 0)  # i.e, Î”z = -K[t]'* Î”u
        mul!(Î”CP, invV[t], Î”K', 1, 1) # Î”CP += invV[t] * Î”K'

        # Î”V .= -invV[t] * CP[t] * Î”K * invV[t]
        mul!(temp_L_N, invV[t], (P_mid[t] * C')')
        mul!(temp_N_L, Î”K, invV[t])
        mul!(Î”V, temp_L_N, temp_N_L, -1, 0)

        mul!(Î”C, Î”CP, P_mid[t]', 1, 1) # Î”C += Î”CP * P_mid[t]'
        mul!(Î”P_mid, C', Î”CP, 1, 1) # Î”P_mid += C' * Î”CP
        mul!(Î”z, invV[t], innovation[t], Î”logpdf, 1) # Î”z += Î”logpdf * invV[t] * innovation[t] # Î£^-1 * (z_obs - z)

        #Î”V -= Î”logpdf * 0.5 * (invV[t] - invV[t] * innovation[t] * innovation[t]' * invV[t]) # -0.5 * (Î£^-1 - Î£^-1(z_obs - z)(z_obx - z)'Î£^-1)
        mul!(temp_M, invV[t], innovation[t])
        mul!(temp_L_L, temp_M, temp_M')
        temp_L_L .-= invV[t]
        rmul!(temp_L_L, Î”logpdf * 0.5)
        Î”V += temp_L_L

        #Î”C += Î”V * C * P_mid[t]' + Î”V' * C * P_mid[t]
        mul!(temp_L_N, C, P_mid[t])
        transpose!(temp_L_L, Î”V)
        temp_L_L .+= Î”V
        mul!(Î”C, temp_L_L, temp_L_N, 1, 1)

        # Î”P_mid += C' * Î”V * C
        mul!(temp_L_N, Î”V, C)
        mul!(Î”P_mid, C', temp_L_N, 1, 1)

        mul!(Î”C, Î”z, u_mid[t]', 1, 1) # Î”C += Î”z * u_mid[t]'
        mul!(Î”u_mid, C', Î”z, 1, 1) # Î”u_mid += C' * Î”z

        # # Calculates (Î”P_mid + Î”P_mid')
        # transpose!(Î”P_mid_sum, Î”P_mid)
        # Î”P_mid_sum .+= Î”P_mid

        # Î”A += (Î”P_mid + Î”P_mid') * A * P[t - 1]
        mul!(temp_N_N, A, P[t - 1])
        # mul!(Î”A, Î”P_mid_sum, temp_N_N, 1, 1)
        mul!(Î”A, Î”P_mid, temp_N_N, 1, 1)

        # Î”P .= A' * Î”P_mid * A # pass into next period
        mul!(temp_N_N, Î”P_mid, A)
        mul!(Î”P, A', temp_N_N)

        # mul!(Î”B, Î”P_mid_sum, B, 1, 1) # Î”B += Î”P_mid_sum * B
        mul!(Î”B, Î”P_mid, B, 1, 1) # Î”B += Î”P_mid_sum * B
        mul!(Î”A, Î”u_mid, u[t - 1]', 1, 1) # Î”A += Î”u_mid * u[t - 1]'
        mul!(Î”u, A', Î”u_mid)
    end




    # reverse pass new but old order
    Î”logpdf = 1.0
    temp_L_N = similar(C)
    temp_N_L = similar(C')
    temp_L_L = similar(V[1])
    temp_M = similar(z[1])

    # Buffers
    Î”P = zero(P[1])
    Î”u = zero(u[1])
    Î”A = zero(A)
    Î”B = zero(P[1])
    Î”C = zero(C)
    Î”K = zero(K[1])
    Î”P_mid = zero(Î”P)
    Î”P_mid_sum = zero(Î”P)
    Î”CP = zero(CP[1])
    Î”PC = zero(CP[1])
    Î”u_mid = zero(u_mid[1])
    Î”z = zero(z[1])
    Î”V = zero(V[1])
    

    for t in T:-1:2
        # pullback
        # Sensitivity accumulation
        # P[t] -= K[t] * CP[t]
        copy!(Î”P_mid, Î”P)
        Î”K .= -Î”P * CP[t]'
        Î”CP .= - K[t]' * Î”P

        # u[t] += K[t] * innovation[t]
        copy!(Î”u_mid, Î”u)
        Î”K += Î”u * innovation[t]'
        Î”innovation = K[t]'* Î”u

        # K[t] .= CP[t]' / V[t]
        Î”PC .= invV[t] * Î”K'
        Î”V .= -invV[t] * (P_mid[t] * C')' * Î”K * invV[t]
        
        # PC = P_mid[t] * C'
        Î”P_mid += Î”PC' * C
        Î”C .= (P_mid[t] * Î”PC')'

        # loglik += log(Vdet) + innovation[t]' * invV[t] * innovation[t]
        Î”innovation += 2 * Î”logpdf * invV[t] * innovation[t] # Î£^-1 * (z_obs - z)
        Î”V -= Î”logpdf * (invV[t] - invV[t] * innovation[t] * innovation[t]' * invV[t])

        # innovation[t] .= observables[:, t-1] - z[t-1]
        Î”observables = Î”innovation
        Î”z = -Î”innovation

        # V[t] = CP[t] * C' + R
        Î”C .= Î”V * C * P_mid[t]'# + Î”V' * C * P_mid[t]
        Î”P_mid += C' * Î”V * C

        # CP[t] = C * P[t]
        Î”C += Î”CP * P_mid[t]'
        Î”P_mid += C' * Î”CP

        # P[t] = A * P[t - 1] * A' + B
        Î”A .= Î”P_mid * A * P[t - 1]
        Î”P .= A' * Î”P_mid * A # pass into next period
        Î”B .= Î”P_mid

        # z[t] = C * u[t]
        Î”C += Î”z * u_mid[t]'
        Î”u_mid += C' * Î”z

        # u[t] = A * u[t-1]
        Î”A += Î”u_mid * u[t - 1]'
        Î”u = A' * Î”u_mid
    end
    t -= 1

    Î”A






        # If it was a failure, just return and hope the gradients are ignored!
            for t in T:-1:2
                # # Calculates (Î”P_mid + Î”P_mid')
                # transpose!(Î”P_mid_sum, Î”P_mid)
                # Î”P_mid_sum .+= Î”P_mid

                # Î”A += (Î”P_mid + Î”P_mid') * A * P[t - 1]
                mul!(temp_N_N, A, P[t])
                mul!(Î”A, Î”P_mid, temp_N_N, 1, 1)

                # Î”P .= A' * Î”P_mid * A # pass into next period
                mul!(temp_N_N, Î”P_mid, A)
                mul!(Î”P, A', temp_N_N)

                mul!(Î”B, Î”P_mid, B, 1, 1) # Î”B += Î”P_mid_sum * B
                mul!(Î”A, Î”u_mid, u[t]', 1, 1) # Î”A += Î”u_mid * u[t - 1]'
                mul!(Î”u, A', Î”u_mid)

                # Sensitivity accumulation
                copy!(Î”P_mid, Î”P)
                mul!(Î”K, Î”P, CP[t]', -1, 0) # i.e. Î”K = -Î”P * CP[t]'
                mul!(Î”CP, K[t]', Î”P, -1, 0) # i.e. Î”CP = - K[t]' * Î”P
                copy!(Î”u_mid, Î”u)
                mul!(Î”K, Î”u, innovation[t]', 1, 1) # Î”K += Î”u * innovation[t]'
                mul!(Î”z, K[t]', Î”u, -1, 0)  # i.e, Î”z = -K[t]'* Î”u
                mul!(Î”CP, invV[t], Î”K', 1, 1) # Î”CP += invV[t] * Î”K'

                # Î”V .= -invV[t] * CP[t] * Î”K * invV[t]
                mul!(temp_L_N, invV[t], CP[t])
                mul!(temp_N_L, Î”K, invV[t])
                mul!(Î”V, temp_L_N, temp_N_L, -1, 0)

                # mul!(Î”C, Î”CP, P_mid[t]', 1, 1) # Î”C += Î”CP * P_mid[t]'
                mul!(Î”P_mid, C', Î”CP, 1, 1) # Î”P_mid += C' * Î”CP
                mul!(Î”z, invV[t], innovation[t], Î”logpdf, 1) # Î”z += Î”logpdf * invV[t] * innovation[t] # Î£^-1 * (z_obs - z)

                #Î”V -= Î”logpdf * 0.5 * (invV[t] - invV[t] * innovation[t] * innovation[t]' * invV[t]) # -0.5 * (Î£^-1 - Î£^-1(z_obs - z)(z_obx - z)'Î£^-1)
                mul!(temp_M, invV[t], innovation[t])
                mul!(temp_L_L, temp_M, temp_M')
                temp_L_L .-= invV[t]
                rmul!(temp_L_L, Î”logpdf * 0.5)
                Î”V += temp_L_L

                #Î”C += Î”V * C * P_mid[t]' + Î”V' * C * P_mid[t]
                mul!(temp_L_N, C, P_mid[t])
                transpose!(temp_L_L, Î”V)
                temp_L_L .+= Î”V
                # mul!(Î”C, temp_L_L, temp_L_N, 1, 1)

                # Î”P_mid += C' * Î”V * C
                mul!(temp_L_N, Î”V, C)
                mul!(Î”P_mid, C', temp_L_N, 1, 1)

                # mul!(Î”C, Î”z, u_mid[t]', 1, 1) # Î”C += Î”z * u_mid[t]'
                mul!(Î”u_mid, C', Î”z, 1, 1) # Î”u_mid += C' * Î”z
            end









P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)

u = zeros(size(C,2))

loglik = 0.0

# single update function
function kalman_iteration(u, P, loglik, A, ğ, C, data_in_deviations, presample_periods, t)
    v = data_in_deviations[:, t] - C * u

    F = C * P * C'

    luF = â„’.lu(F, check = false) ###

    if !â„’.issuccess(luF)
        return -Inf
    end

    Fdet = â„’.det(luF)

    if Fdet < eps(Float64)
        return -Inf
    end

    invF = inv(luF) ###
    

    if t > presample_periods
        llh = loglik +logdet(F) + v' * invF * v ###
    else
        llh = loglik
    end

    uÌ‚ = A * (u + P * C' * invF * v)

    PÌ‚ = A * (P - P * C' * invF * C * P) * A' + ğ

    return uÌ‚, PÌ‚, llh
end

for t in 1:size(data_in_deviations, 2)
    u,P,loglik = kalman_update(u,P,loglik,A,ğ,C,data_in_deviations,presample_periods,t)
end

-(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 






P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)

u = zeros(size(C,2))

loglik = 0.0

v = data_in_deviations[:, t] - C * u
        
F = C * P * C'

luF = â„’.lu(F, check = false) ###

if !â„’.issuccess(luF)
    return -Inf
end

Fdet = â„’.det(luF)

if Fdet < eps(Float64)
    return -Inf
end

invF = inv(luF) ###


if t > presample_periods
    llh = loglik +logdet(F) + v' * invF * v ###
else
    llh = loglik
end

uÌ‚ = A * (u + P * C' * invF * v)

PÌ‚ = A * (P - P * C' * invF * C * P) * A' + ğ




function rrule(::typeof(kalman_iteration), u, P, loglik, A, ğ, C, data_in_deviations, presample_periods, t)
    # Perform the forward pass
    v = data_in_deviations[:, t] - C * u

    F = C * P * C'

    luF = â„’.lu(F, check = false) ###

    if !â„’.issuccess(luF)
        return (u, P, -Inf), (u, P, loglik, A, ğ, C, data_in_deviations, presample_periods, t) -> (u, P, loglik)
    end

    Fdet = â„’.det(luF)

    if Fdet < eps(Float64)
        return (u, P, -Inf), (u, P, loglik, A, ğ, C, data_in_deviations, presample_periods, t) -> (u, P, loglik)
    end

    invF = inv(luF) ###
    
    if t > presample_periods
        llh = loglik +logdet(F) + v' * invF * v ###
    else
        llh = loglik
    end

    uÌ‚ = A * (u + P * C' * invF * v)

    PÌ‚ = A * (P - P * C' * invF * C * P) * A' + ğ

    # pullback of single update function
    function kalman_pullback(âˆ‚Ã», âˆ‚PÌ‚, âˆ‚llh)
        # Calculate gradients for each input
        âˆ‚loglik = âˆ‚llh
    
        # Gradient w.r.t. P from `PÌ‚`
        âˆ‚P = -A' * âˆ‚PÌ‚ * A  # Simplified reverse derivative, expand with chain rule for full gradient
    
        # Additional contributions to âˆ‚P from llh
        if t > presample_periods
            âˆ‚S_llh = C' * (âˆ‚llh * invF - invF * (v * v') * invF) * C
            âˆ‚P += âˆ‚S_llh
        end
        
        # Gradient w.r.t. u
        âˆ‚u = -C' * invF * v  # Derivative contribution from v in the update step
        âˆ‚u += A' * âˆ‚Ã»
        
        # Gradient w.r.t. A
        âˆ‚A = âˆ‚Ã» * (u + P * C' * invF * v)' + âˆ‚PÌ‚ * (P - P * C' * invF * C * P)'
        
        # Gradient w.r.t. B
        âˆ‚B = âˆ‚PÌ‚
    
        return NoTangent(), âˆ‚u, âˆ‚P, âˆ‚loglik, âˆ‚A, âˆ‚B, NoTangent(), NoTangent(), NoTangent(), NoTangent()
    end
    
    return (uÌ‚, PÌ‚, llh), kalman_pullback
end


# test
using ForwardDiff, Zygote, FiniteDifferences


fordif = ForwardDiff.gradient(x->begin
                P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)

                u = zeros(size(C,2))

                loglik = 0.0

                for t in 1:size(data_in_deviations, 2)
                    u,P,loglik = kalman_update(u,P,loglik,x,ğ,C,data_in_deviations,presample_periods,t)
                end

                -(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 
                end, A)


findif = FiniteDifferences.grad(central_fdm(3,1), x->begin
                P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)

                u = zeros(size(C,2))

                loglik = 0.0

                for t in 1:size(data_in_deviations, 2)
                    u,P,loglik = kalman_update(u,P,loglik,x,ğ,C,data_in_deviations,presample_periods,t)
                end

                -(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 
                end, A)[1]


bacdif = Zygote.gradient(x->begin
                P = get_initial_covariance(Val(initial_covariance), values, coordinates, dimensions)

                u = zeros(size(C,2))

                loglik = 0.0

                for t in 1:size(data_in_deviations, 2)
                    u,P,loglik = kalman_update(u,P,loglik,x,ğ,C,data_in_deviations,presample_periods,t)
                end

                -(loglik + ((size(data_in_deviations, 2) - presample_periods) * size(data_in_deviations, 1)) * log(2 * 3.141592653589793)) / 2 
                end, A)[1]
