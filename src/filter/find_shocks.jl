# Algorithms
# - LagrangeNewton: fast, but no guarantee of convergence to global minimum
# - COBYLA: best known chances of convergence to global minimum; ok speed for third order; lower tol on optimality conditions (1e-7)
# - SLSQP: relatively slow and not guaranteed to converge to global minimum

# Generalized find_shocks for conditional forecasts
# This function finds shocks that minimize their squared magnitude while satisfying 
# conditional forecast constraints (only some variables match target values)
# Uses analytical derivatives from perturbation solution matrices (like find_shocks)

@stable default_mode = "disable" begin
function find_shocks_conditional_forecast(::Val{:LagrangeNewton},
                                         state_update::Function,
                                         initial_state::Union{Vector{Float64}, Vector{Vector{Float64}}},
                                         all_shocks::Vector{Float64},
                                         conditions::Vector{Float64},
                                         cond_var_idx::Vector{Int},
                                         free_shock_idx::Vector{Int},
                                         pruning::Bool,
                                         ğ’Â¹áµ‰::AbstractMatrix{Float64},  # Shock columns from first-order solution
                                         ğ’Â²áµ‰::Union{AbstractMatrix{Float64}, Nothing},  # Second-order solution matrix
                                         ğ’Â³áµ‰::Union{AbstractMatrix{Float64}, Nothing},  # Third-order solution matrix
                                         T::timings;
                                         max_iter::Int = 1000,
                                         tol::Float64 = 1e-13)
    # For underdetermined systems (more shocks than conditions), go straight to LM
    # as it handles these cases better
    if length(free_shock_idx) > length(cond_var_idx)
        # Use fewer iterations for underdetermined systems as each iteration is expensive
        max_iter_adjusted = min(max_iter, 100)  # Cap at 100 iterations
        
        # Try LM directly for underdetermined systems
        x, converged = find_shocks_conditional_forecast_core(
            state_update, initial_state, all_shocks, conditions,
            cond_var_idx, free_shock_idx, pruning,
            ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T;
            max_iter=max_iter_adjusted, tol=tol, use_globalization=false, use_levenberg_marquardt=true, use_continuation=false)
        
        if !converged
            # Last resort: try with very relaxed tolerance
            x, converged = find_shocks_conditional_forecast_core(
                state_update, initial_state, all_shocks, conditions,
                cond_var_idx, free_shock_idx, pruning,
                ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T;
                max_iter=max_iter_adjusted, tol=tol*100, use_globalization=false, use_levenberg_marquardt=true, use_continuation=false)
        end
        
        return x, converged
    end
    
    # For determined/overdetermined systems, use standard cascading strategy
    # First try without globalization (faster)
    x, converged = find_shocks_conditional_forecast_core(
        state_update, initial_state, all_shocks, conditions,
        cond_var_idx, free_shock_idx, pruning,
        ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T;
        max_iter=max_iter, tol=tol, use_globalization=false, use_levenberg_marquardt=false, use_continuation=false)
    
    # If failed, try with line search globalization
    if !converged
        x, converged = find_shocks_conditional_forecast_core(
            state_update, initial_state, all_shocks, conditions,
            cond_var_idx, free_shock_idx, pruning,
            ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T;
            max_iter=max_iter, tol=tol, use_globalization=true, use_levenberg_marquardt=false, use_continuation=false)
    end
    
    # If still failed, try Levenberg-Marquardt
    if !converged
        x, converged = find_shocks_conditional_forecast_core(
            state_update, initial_state, all_shocks, conditions,
            cond_var_idx, free_shock_idx, pruning,
            ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T;
            max_iter=max_iter, tol=tol, use_globalization=false, use_levenberg_marquardt=true, use_continuation=false)
    end
    
    return x, converged
end
end # dispatch_doctor


@stable default_mode = "disable" begin
function find_shocks_conditional_forecast_core(
                                         state_update::Function,
                                         initial_state::Union{Vector{Float64}, Vector{Vector{Float64}}},
                                         all_shocks::Vector{Float64},
                                         conditions::Vector{Float64},
                                         cond_var_idx::Vector{Int},
                                         free_shock_idx::Vector{Int},
                                         pruning::Bool,
                                         ğ’Â¹áµ‰::AbstractMatrix{Float64},  # Shock columns from first-order solution
                                         ğ’Â²áµ‰::Union{AbstractMatrix{Float64}, Nothing},  # Second-order solution matrix
                                         ğ’Â³áµ‰::Union{AbstractMatrix{Float64}, Nothing},  # Third-order solution matrix
                                         T::timings;
                                         max_iter::Int = 1000,
                                         tol::Float64 = 1e-13,
                                         use_globalization::Bool = false,
                                         use_levenberg_marquardt::Bool = false,
                                         use_continuation::Bool = false)
    
    # Pure Lagrange-Newton: when no globalization methods are enabled
    pure_newton = !use_globalization && !use_levenberg_marquardt && !use_continuation
    
    # Track improvement for pure Newton
    last_residual_norm = Inf
    stalled_count = 0
    
    # Initialize free shocks
    # For underdetermined systems (more shocks than conditions), use pseudoinverse for better initial guess
    if length(free_shock_idx) > length(cond_var_idx) && !use_levenberg_marquardt && !use_continuation
        # Get initial Jacobian (linear part)
        jacobian_init = -ğ’Â¹áµ‰[cond_var_idx, free_shock_idx]
        
        # Compute initial state
        new_state_init = state_update(initial_state, all_shocks)
        cond_vars_init = pruning ? sum(new_state_init) : new_state_init
        residual_init = conditions - cond_vars_init[cond_var_idx]
        
        # Use pseudoinverse to get minimum norm solution as initial guess
        # x = J^+ * residual where J^+ is pseudoinverse
        try
            x = â„’.pinv(jacobian_init) * residual_init
            # Limit initial guess to reasonable range
            x = clamp.(x, -5.0, 5.0)  # Tighter bounds for better stability
        catch
            x = zeros(length(free_shock_idx))
        end
    elseif use_levenberg_marquardt || use_continuation
        # For LM and continuation, use smaller initial shocks for stability
        x = zeros(length(free_shock_idx))
    else
        x = zeros(length(free_shock_idx))
    end
    
    # For continuation method - DISABLED: too slow with recursive calls
    # Left as placeholder for future non-recursive implementation
    if use_continuation
        # Currently disabled - just use zero initialization
        x = zeros(length(free_shock_idx))
    end
    
    # Lagrange multipliers for equality constraints
    Î» = zeros(length(cond_var_idx))
    
    xÎ» = vcat(x, Î»)
    Î”xÎ» = copy(xÎ»)
    
    norm1 = â„’.norm(conditions)
    norm2 = 1.0
    
    # Pre-allocate buffers
    residual = zeros(length(cond_var_idx))
    jacobian = zeros(length(cond_var_idx), length(free_shock_idx))
    fxÎ» = zeros(length(xÎ»))
    fxÎ»p = zeros(length(xÎ»), length(xÎ»))
    
    lI = -2.0 * â„’.I(length(free_shock_idx))
    
    # Buffers for analytical derivative computation  
    J = â„’.Diagonal(ones(Bool, length(all_shocks)))
    kron_buffer = zeros(length(all_shocks) * length(all_shocks))
    kron_buffer2 = â„’.kron(J, zeros(length(all_shocks)))  # Initialize with correct dimensions
    kron_buffer3 = â„’.kron(J, kron_buffer)  # Initialize with correct dimensions for third-order
    âˆ‚x = zero(ğ’Â¹áµ‰)
    
    # For globalization and Levenberg-Marquardt
    prev_merit = Inf
    if use_globalization
        xÎ»_temp = copy(xÎ»)
    end
    
    # Levenberg-Marquardt damping parameter
    # Start with moderate damping for underdetermined systems
    # Larger initial damping for better robustness
    Î¼ = length(free_shock_idx) > length(cond_var_idx) ? 1.0 : 0.1
    Î½ = 3.0  # Scaling factor for damping updates
    
    # Debug flag
    debug = length(free_shock_idx) > length(cond_var_idx) && use_levenberg_marquardt
    
    @inbounds for iter in 1:max_iter
        if debug && iter <= 5
            println("  LM iter $iter: ||x|| = $(â„’.norm(x)), Î¼ = $Î¼")
        end
        
        # Update all shocks with current free shock values
        all_shocks[free_shock_idx] .= x
        
        # Compute new state
        new_state = state_update(initial_state, all_shocks)
        cond_vars = pruning ? sum(new_state) : new_state
        
        # Compute residual: target - actual
        residual .= conditions - cond_vars[cond_var_idx]
        
        # Compute Jacobian analytically using perturbation matrices
        # Following the same pattern as find_shocks
        # âˆ‚y/âˆ‚Îµ = ğ’Â¹áµ‰ + 2*ğ’Â²áµ‰*kron(I, Îµ) + 3*ğ’Â³áµ‰*kron(I, kron(Îµ, Îµ))
        
        if !isnothing(ğ’Â³áµ‰)
            # Third-order: analytical Jacobian with cubic term
            # âˆ‚x = ğ’Â¹áµ‰ + 2 * ğ’Â²áµ‰ * kron(I, all_shocks) + 3 * ğ’Â³áµ‰ * kron(I, kron(all_shocks, all_shocks))
            â„’.kron!(kron_buffer, all_shocks, all_shocks)
            â„’.kron!(kron_buffer2, J, all_shocks)
            â„’.kron!(kron_buffer3, J, kron_buffer)
            
            copy!(âˆ‚x, ğ’Â¹áµ‰)
            â„’.mul!(âˆ‚x, ğ’Â²áµ‰, kron_buffer2, 2, 1)
            â„’.mul!(âˆ‚x, ğ’Â³áµ‰, kron_buffer3, 3, 1)
            
            # Extract rows for conditioned variables and columns for free shocks
            jacobian .= -âˆ‚x[cond_var_idx, free_shock_idx]
        elseif !isnothing(ğ’Â²áµ‰)
            # Second-order: analytical Jacobian with quadratic term
            # âˆ‚x = ğ’Â¹áµ‰ + 2 * ğ’Â²áµ‰ * kron(I, all_shocks)
            â„’.kron!(kron_buffer2, J, all_shocks)
            â„’.mul!(âˆ‚x, ğ’Â²áµ‰, kron_buffer2)
            â„’.axpby!(1, ğ’Â¹áµ‰, 2, âˆ‚x)
            
            # Extract rows for conditioned variables and columns for free shocks
            jacobian .= -âˆ‚x[cond_var_idx, free_shock_idx]
        else
            # First-order: just use ğ’Â¹áµ‰
            jacobian .= -ğ’Â¹áµ‰[cond_var_idx, free_shock_idx]
        end
        
        # Build KKT system
        # First order optimality: gradient of Lagrangian wrt x
        fxÎ»[1:length(x)] .= 2.0 * x + jacobian' * Î»
        
        # Equality constraints
        fxÎ»[length(x)+1:end] .= residual
        
        # Build Jacobian of KKT system
        fxÎ»p[1:length(x), 1:length(x)] .= lI
        fxÎ»p[1:length(x), length(x)+1:end] .= jacobian'
        fxÎ»p[length(x)+1:end, 1:length(x)] .= jacobian
        fxÎ»p[length(x)+1:end, length(x)+1:end] .= 0.0
        
        # Apply Levenberg-Marquardt damping if enabled
        if use_levenberg_marquardt
            # Add damping to the Hessian block: (H + Î¼I)
            for i in 1:length(x)
                fxÎ»p[i, i] -= 2.0 * Î¼  # Subtract 2Î¼ because lI = -2I already
            end
        end
        
        # Solve Newton step
        try
            fÌ‚xÎ»p = â„’.factorize(fxÎ»p)
            â„’.ldiv!(Î”xÎ», fÌ‚xÎ»p, fxÎ»)
        catch
            if use_levenberg_marquardt && Î¼ < 1e10
                # Try with larger damping
                Î¼ *= Î½
                continue
            end
            return x, false
        end
        
        if !all(isfinite, Î”xÎ»)
            break
        end
        
        # Update with Levenberg-Marquardt adaptive damping
        if use_levenberg_marquardt
            # Compute current cost: ||x||^2 + ||residual||^2
            current_cost = â„’.dot(x, x) + â„’.dot(residual, residual)
            
            # Try the step
            xÎ»_trial = xÎ» - Î”xÎ»
            x_trial = xÎ»_trial[1:length(x)]
            Î»_trial = xÎ»_trial[length(x)+1:end]
            
            # Compute actual reduction
            all_shocks[free_shock_idx] .= x_trial
            new_state_trial = state_update(initial_state, all_shocks)
            cond_vars_trial = pruning ? sum(new_state_trial) : new_state_trial
            residual_trial = conditions - cond_vars_trial[cond_var_idx]
            
            trial_cost = â„’.dot(x_trial, x_trial) + â„’.dot(residual_trial, residual_trial)
            actual_reduction = current_cost - trial_cost
            
            # Predicted reduction from linear model
            # For LM: F(x+h) â‰ˆ F(x) + J*h + 0.5*h'*H*h where H includes damping
            # Here we use simplified predicted reduction
            predicted_reduction = -â„’.dot(fxÎ», Î”xÎ») - 0.5 * Î¼ * â„’.dot(Î”xÎ»[1:length(x)], Î”xÎ»[1:length(x)])
            
            # Compute gain ratio
            # Avoid division by very small numbers
            if abs(predicted_reduction) < 1e-20
                Ï = actual_reduction > 0 ? 1.0 : -1.0
            else
                Ï = actual_reduction / predicted_reduction
            end
            
            # More lenient acceptance criterion and better damping strategy
            if Ï > 0.0  # Accept any improvement
                xÎ» .= xÎ»_trial
                x .= x_trial
                Î» .= Î»_trial
                
                # Update damping parameter based on gain ratio
                if Ï > 0.75  # Very good agreement with model
                    Î¼ = max(Î¼ / Î½, 1e-12)  # Reduce damping (getting closer to Newton)
                elseif Ï > 0.25  # Reasonable agreement
                    Î¼ = max(Î¼ / 2, 1e-12)  # Moderately reduce damping
                elseif Ï < 0.1  # Poor agreement  
                    Î¼ = min(Î¼ * Î½, 1e8)    # Increase damping
                end
                # else: keep Î¼ unchanged for moderate progress
                
            else  # Reject step, increase damping
                Î¼ = min(Î¼ * Î½, 1e8)
                if Î¼ > 1e6  # Damping too large, algorithm stuck
                    if debug
                        println("  LM stopped: damping too large (Î¼ = $Î¼)")
                    end
                    break
                end
                continue  # Don't update x, Î», try again with larger damping
            end
            
        # Update with line search globalization if enabled
        elseif use_globalization
            # Try multiple merit function formulations
            # Merit 1: Standard L2 penalty ||x||^2 + penalty * ||residual||^2
            # Merit 2: L1 penalty ||x||_1 + penalty * ||residual||_1  (more robust to outliers)
            # Merit 3: Fletcher penalty with adaptive weight
            
            # Adaptive penalty based on problem conditioning
            # Higher penalty for underdetermined systems to emphasize constraint satisfaction
            base_penalty = length(free_shock_idx) > length(cond_var_idx) ? 500.0 : 100.0
            
            # Try different merit functions
            best_Î± = 0.0
            best_merit = Inf
            best_x = copy(x)
            
            for merit_type in [:l2_quadratic, :l1_robust, :adaptive_fletcher]
                # Compute current merit
                if merit_type == :l2_quadratic
                    penalty = base_penalty
                    current_merit = â„’.dot(x, x) + penalty * â„’.dot(residual, residual)
                elseif merit_type == :l1_robust
                    penalty = base_penalty
                    current_merit = â„’.norm(x, 1) + penalty * â„’.norm(residual, 1)
                else  # adaptive_fletcher
                    # Fletcher's merit: ||x||^2 + Î»'*residual + 0.5*penalty*||residual||^2
                    penalty = base_penalty * (1.0 + iter / max_iter)  # Increase penalty over time
                    current_merit = â„’.dot(x, x) + â„’.dot(Î», residual) + 0.5 * penalty * â„’.dot(residual, residual)
                end
                
                # Line search: try step sizes Î± = 1, 0.5, 0.25, 0.125, ...
                Î± = 1.0
                xÎ»_temp .= xÎ»
                
                for ls_iter in 1:12  # Try up to 12 backtracking steps
                    xÎ»_temp .= xÎ» - Î± * Î”xÎ»
                    x_temp = xÎ»_temp[1:length(free_shock_idx)]
                    Î»_temp = xÎ»_temp[length(free_shock_idx)+1:end]
                    
                    # Evaluate merit at trial point
                    all_shocks[free_shock_idx] .= x_temp
                    new_state_temp = state_update(initial_state, all_shocks)
                    cond_vars_temp = pruning ? sum(new_state_temp) : new_state_temp
                    residual_temp = conditions - cond_vars_temp[cond_var_idx]
                    
                    if merit_type == :l2_quadratic
                        trial_merit = â„’.dot(x_temp, x_temp) + penalty * â„’.dot(residual_temp, residual_temp)
                    elseif merit_type == :l1_robust
                        trial_merit = â„’.norm(x_temp, 1) + penalty * â„’.norm(residual_temp, 1)
                    else  # adaptive_fletcher
                        trial_merit = â„’.dot(x_temp, x_temp) + â„’.dot(Î»_temp, residual_temp) + 0.5 * penalty * â„’.dot(residual_temp, residual_temp)
                    end
                    
                    # Track best across all merit functions and step sizes
                    if trial_merit < best_merit
                        best_merit = trial_merit
                        best_Î± = Î±
                        best_x .= x_temp
                    end
                    
                    # Sufficient decrease condition (Armijo rule with adaptive c)
                    c = merit_type == :l1_robust ? 1e-3 : 1e-4  # More lenient for L1
                    if trial_merit < current_merit - c * Î± * â„’.dot(Î”xÎ», Î”xÎ»)
                        # Found acceptable step for this merit function
                        if best_Î± == 0.0 || Î± > best_Î±
                            best_Î± = Î±
                            best_x .= x_temp
                        end
                        break
                    end
                    
                    Î± *= 0.5
                end
            end
            
            # Use best step found across all merit functions
            if best_Î± > 0.0
                x .= best_x
                # Update full xÎ» vector
                xÎ»[1:length(free_shock_idx)] .= x
                # Recompute Î» with current x
                all_shocks[free_shock_idx] .= x
                new_state = state_update(initial_state, all_shocks)
                cond_vars = pruning ? sum(new_state) : new_state
                residual .= conditions - cond_vars[cond_var_idx]
                # Don't update Î» here - will be updated in next Newton step
            else
                # No improvement found with any merit function, take very small step
                xÎ» .-= 0.005 * Î”xÎ»
                x .= xÎ»[1:length(free_shock_idx)]
            end
        else
            # Standard Newton update without globalization
            xÎ» .-= Î”xÎ»
        end
        
        x .= xÎ»[1:length(free_shock_idx)]
        Î» .= xÎ»[length(free_shock_idx)+1:end]
        
        # Check convergence
        norm2 = â„’.norm(cond_vars[cond_var_idx])
        residual_norm = â„’.norm(residual) / max(norm1, norm2)
        step_norm = â„’.norm(Î”xÎ») / max(â„’.norm(xÎ»), 1.0)
        
        if debug && iter <= 5
            println("    residual_norm = $residual_norm, step_norm = $step_norm, tol = $tol")
        end
        
        # For pure Newton: detect stalling
        if pure_newton
            improvement = last_residual_norm - residual_norm
            if improvement < tol * 0.01  # Not making meaningful progress
                stalled_count += 1
            else
                stalled_count = 0  # Reset if made progress
            end
            last_residual_norm = residual_norm
            
            # If stalled for 3 consecutive iterations, apply iterative refinement
            if stalled_count >= 3 || iter == max_iter
                if debug
                    if stalled_count >= 3
                        println("  Pure Newton stalled after $iter iterations")
                    else
                        println("  Pure Newton reached max_iter")
                    end
                    println("  Applying iterative refinement...")
                end
                
                # Apply iterative refinement
                x_refined, improved = iterative_refinement(
                    x, state_update, initial_state, all_shocks, conditions,
                    cond_var_idx, free_shock_idx, pruning,
                    ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T, jacobian, âˆ‚x, kron_buffer2, kron_buffer3, J;
                    max_refine_iter=10, tol=tol, debug=debug)
                
                # Check if refinement achieved convergence
                all_shocks[free_shock_idx] .= x_refined
                new_state_final = state_update(initial_state, all_shocks)
                cond_vars_final = pruning ? sum(new_state_final) : new_state_final
                residual_final = conditions - cond_vars_final[cond_var_idx]
                norm2_final = â„’.norm(cond_vars_final[cond_var_idx])
                residual_norm_final = â„’.norm(residual_final) / max(norm1, norm2_final)
                
                converged_after_refinement = residual_norm_final < tol
                
                if converged_after_refinement && debug
                    println("  âœ“ Converged after iterative refinement! (residual_norm = $residual_norm_final)")
                elseif improved && debug
                    println("  Iterative refinement improved solution (residual_norm: $residual_norm â†’ $residual_norm_final)")
                end
                
                return x_refined, converged_after_refinement
            end
        end
        
        if residual_norm < tol && step_norm < sqrt(tol)
            if debug
                println("  Converged in $iter iterations!")
            end
            return x, true
        end
    end
    
    if debug
        println("  Did NOT converge after $max_iter iterations")
    end
    
    # For non-pure Newton methods, also try iterative refinement as final attempt
    if !pure_newton
        if debug
            println("  Attempting iterative refinement as final polish...")
        end
        
        x_refined, improved = iterative_refinement(
            x, state_update, initial_state, all_shocks, conditions,
            cond_var_idx, free_shock_idx, pruning,
            ğ’Â¹áµ‰, ğ’Â²áµ‰, ğ’Â³áµ‰, T, jacobian, âˆ‚x, kron_buffer2, kron_buffer3, J;
            max_refine_iter=10, tol=tol, debug=debug)
        
        if improved && debug
            println("  Iterative refinement improved solution")
        end
        
        # Check if refinement achieved convergence
        all_shocks[free_shock_idx] .= x_refined
        new_state_final = state_update(initial_state, all_shocks)
        cond_vars_final = pruning ? sum(new_state_final) : new_state_final
        residual_final = conditions - cond_vars_final[cond_var_idx]
        norm2_final = â„’.norm(cond_vars_final[cond_var_idx])
        residual_norm_final = â„’.norm(residual_final) / max(norm1, norm2_final)
        
        converged_after_refinement = residual_norm_final < tol
        
        if converged_after_refinement && debug
            println("  Converged after iterative refinement!")
        end
        
        return x_refined, converged_after_refinement
    end
    
    return x, false
end
end # dispatch_doctor


# Iterative refinement: polish the solution by solving for the residual
# This can improve precision when the main algorithm has stalled
@stable default_mode = "disable" begin
function iterative_refinement(
    x::Vector{Float64},
    state_update::Function,
    initial_state::Union{Vector{Float64}, Vector{Vector{Float64}}},
    all_shocks::Vector{Float64},
    conditions::Vector{Float64},
    cond_var_idx::Vector{Int},
    free_shock_idx::Vector{Int},
    pruning::Bool,
    ğ’Â¹áµ‰::AbstractMatrix{Float64},
    ğ’Â²áµ‰::Union{AbstractMatrix{Float64}, Nothing},
    ğ’Â³áµ‰::Union{AbstractMatrix{Float64}, Nothing},
    T::timings,
    jacobian::Matrix{Float64},
    âˆ‚x::Matrix{Float64},
    kron_buffer2::Matrix{Float64},
    kron_buffer3::Matrix{Float64},
    J::â„’.Diagonal{Bool, Vector{Bool}};
    max_refine_iter::Int = 5,
    tol::Float64 = 1e-13,
    debug::Bool = false)
    
    x_current = copy(x)
    best_residual_norm = Inf
    improved = false
    
    # Compute initial residual
    all_shocks[free_shock_idx] .= x_current
    new_state = state_update(initial_state, all_shocks)
    cond_vars = pruning ? sum(new_state) : new_state
    residual = conditions - cond_vars[cond_var_idx]
    norm1 = â„’.norm(conditions)
    norm2 = â„’.norm(cond_vars[cond_var_idx])
    initial_residual_norm = â„’.norm(residual) / max(norm1, norm2)
    best_residual_norm = initial_residual_norm
    
    if debug
        println("  Iterative refinement starting with residual_norm = $initial_residual_norm")
    end
    
    # Iterative refinement loop
    for refine_iter in 1:max_refine_iter
        # Compute Jacobian at current point
        if !isnothing(ğ’Â³áµ‰)
            # Third-order
            kron_buffer = zeros(length(all_shocks) * length(all_shocks))
            â„’.kron!(kron_buffer, all_shocks, all_shocks)
            â„’.kron!(kron_buffer2, J, all_shocks)
            â„’.kron!(kron_buffer3, J, kron_buffer)
            
            copy!(âˆ‚x, ğ’Â¹áµ‰)
            â„’.mul!(âˆ‚x, ğ’Â²áµ‰, kron_buffer2, 2, 1)
            â„’.mul!(âˆ‚x, ğ’Â³áµ‰, kron_buffer3, 3, 1)
            
            jacobian .= -âˆ‚x[cond_var_idx, free_shock_idx]
        elseif !isnothing(ğ’Â²áµ‰)
            # Second-order
            â„’.kron!(kron_buffer2, J, all_shocks)
            â„’.mul!(âˆ‚x, ğ’Â²áµ‰, kron_buffer2)
            â„’.axpby!(1, ğ’Â¹áµ‰, 2, âˆ‚x)
            
            jacobian .= -âˆ‚x[cond_var_idx, free_shock_idx]
        else
            # First-order
            jacobian .= -ğ’Â¹áµ‰[cond_var_idx, free_shock_idx]
        end
        
        # Solve for correction: J * Î´x = residual
        # Use least-squares for robustness
        Î´x = try
            # Try direct solve first
            jacobian \ residual
        catch
            # Fall back to pseudoinverse if singular
            â„’.pinv(jacobian) * residual
        end
        
        # Apply damped correction to avoid overshooting
        # Start with full step, reduce if it doesn't improve
        accepted_damping = 0.0
        for damping_factor in [1.0, 0.5, 0.25, 0.1]
            x_trial = x_current + damping_factor * Î´x
            
            # Clamp to reasonable bounds
            x_trial .= clamp.(x_trial, -10.0, 10.0)
            
            # Evaluate residual at trial point
            all_shocks[free_shock_idx] .= x_trial
            new_state_trial = state_update(initial_state, all_shocks)
            cond_vars_trial = pruning ? sum(new_state_trial) : new_state_trial
            residual_trial = conditions - cond_vars_trial[cond_var_idx]
            
            norm2_trial = â„’.norm(cond_vars_trial[cond_var_idx])
            residual_norm_trial = â„’.norm(residual_trial) / max(norm1, norm2_trial)
            
            # Accept if improved
            if residual_norm_trial < best_residual_norm
                x_current .= x_trial
                residual .= residual_trial
                cond_vars .= cond_vars_trial
                best_residual_norm = residual_norm_trial
                improved = true
                accepted_damping = damping_factor
                break
            end
        end
        
        if debug && accepted_damping > 0
            println("    Refine iter $refine_iter: residual_norm = $best_residual_norm (damping = $accepted_damping)")
        end
        
        # Check if we've achieved target tolerance
        if best_residual_norm < tol
            if debug
                println("  Iterative refinement converged to target tolerance!")
            end
            break
        end
        
        # Check if making progress
        if refine_iter > 1 && best_residual_norm > 0.99 * initial_residual_norm
            # Not making meaningful progress, stop
            if debug
                println("  Iterative refinement stopped (no progress)")
            end
            break
        end
        
        # No accepted step, stop
        if accepted_damping == 0.0
            if debug
                println("  Iterative refinement stopped (no acceptable step)")
            end
            break
        end
    end
    
    if debug && improved
        improvement_factor = initial_residual_norm / best_residual_norm
        println("  Iterative refinement improved residual by factor of $improvement_factor")
    end
    
    return x_current, improved
end
end # dispatch_doctor


@stable default_mode = "disable" begin
function find_shocks(::Val{:LagrangeNewton},
                    initial_guess::Vector{Float64},
                    kron_buffer::Vector{Float64},
                    kron_buffer2::AbstractMatrix{Float64},
                    J::â„’.Diagonal{Bool, Vector{Bool}},
                    ğ’â±::AbstractMatrix{Float64},
                    ğ’â±Â²áµ‰::AbstractMatrix{Float64},
                    shock_independent::Vector{Float64};
                    max_iter::Int = 1000,
                    tol::Float64 = 1e-13) # will fail for higher or lower precision
    x = copy(initial_guess)
    
    Î» = zeros(size(ğ’â±, 1))
    
    xÎ» = [  x
            Î»   ]

    Î”xÎ» = copy(xÎ»)

    norm1 = â„’.norm(shock_independent) 

    norm2 = 1.0
    
    Î”norm = 1e12

    xÌ‚ = copy(shock_independent)

    xÌ„ = zeros(size(ğ’â±,2))

    âˆ‚x = zero(ğ’â±)
    
    fxÎ» = zeros(length(xÎ»))
    
    fxÎ»p = zeros(length(xÎ»), length(xÎ»))

    tmp = zeros(size(ğ’â±, 2) * size(ğ’â±, 2))

    lI = -2 * vec(â„’.I(size(ğ’â±, 2)))

    @inbounds for i in 1:max_iter
        â„’.kron!(kron_buffer2, J, x)

        â„’.mul!(âˆ‚x, ğ’â±Â²áµ‰, kron_buffer2)
        â„’.axpby!(1, ğ’â±, 2, âˆ‚x)

        â„’.mul!(xÌ„, âˆ‚x', Î»)
        
        â„’.axpy!(-2, x, xÌ„)

        copyto!(fxÎ», 1, xÌ„, 1, size(ğ’â±,2))
        copyto!(fxÎ», size(ğ’â±,2) + 1, xÌ‚, 1, size(shock_independent,1))
        
        # fXÎ» = [(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))' * Î» - 2 * x
                # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x))]

        â„’.mul!(tmp, ğ’â±Â²áµ‰', Î»)
        â„’.axpby!(1, lI, 2, tmp)

        fxÎ»p[1:size(ğ’â±, 2), 1:size(ğ’â±, 2)] = tmp
        fxÎ»p[1:size(ğ’â±, 2), size(ğ’â±, 2)+1:end] = âˆ‚x'

        â„’.rmul!(âˆ‚x, -1)
        fxÎ»p[size(ğ’â±, 2)+1:end, 1:size(ğ’â±, 2)] = âˆ‚x

        # fXÎ»p = [reshape(2 * ğ’â±Â²áµ‰' * Î», size(ğ’â±, 2), size(ğ’â±, 2)) - 2*â„’.I(size(ğ’â±, 2))  (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))'
        #         -(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))  zeros(size(ğ’â±, 1),size(ğ’â±, 1))]
        
        # fÌ‚xÎ»p = â„’.lu(fxÎ»p, check = false)

        # if !â„’.issuccess(fÌ‚xÎ»p)
        #     return x, false
        # end

        try
            fÌ‚xÎ»p = â„’.factorize(fxÎ»p)
            â„’.ldiv!(Î”xÎ», fÌ‚xÎ»p, fxÎ»)
        catch
            # â„’.svd(fxÎ»p)
            # println("factorization fails")
            return x, false
        end
        
        if !all(isfinite,Î”xÎ») break end
        
        â„’.axpy!(-1, Î”xÎ», xÎ»)
        # xÎ» -= Î”xÎ»
    
        # x = xÎ»[1:size(ğ’â±, 2)]
        copyto!(x, 1, xÎ», 1, size(ğ’â±,2))

        # Î» = xÎ»[size(ğ’â±, 2)+1:end]
        copyto!(Î», 1, xÎ», size(ğ’â±,2) + 1, length(Î»))

        â„’.kron!(kron_buffer, x, x)

        â„’.mul!(xÌ‚, ğ’â±Â²áµ‰, kron_buffer)

        â„’.mul!(xÌ‚, ğ’â±, x, 1, 1)

        norm2 = â„’.norm(xÌ‚)

        â„’.axpby!(1, shock_independent, -1, xÌ‚)

        if â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < sqrt(tol)
            # println("LagrangeNewton: $i, Tol reached, $x")
            break
        end

        # if i > 500 && â„’.norm(Î”xÎ») > 1e-11 && â„’.norm(Î”xÎ») > Î”norm
        #     # println("LagrangeNewton: $i, Norm increase")
        #     return x, false
        # end
        # # if i == max_iter
        #     println("LagrangeNewton: $i, Max iter reached")
            # println(â„’.norm(Î”xÎ») / â„’.norm(xÎ»))
        # end
    end

    # println(Î»)
    # println("Norm: $(â„’.norm(xÌ‚) / max(norm1,norm2))")
    # println(â„’.norm(Î”xÎ»))
    # println(â„’.norm(Î”xÎ») / â„’.norm(xÎ»))
    # if !(â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < sqrt(tol))
    #     println("Find shocks failed. Norm 1: $(â„’.norm(xÌ‚) / max(norm1,norm2)); Norm 2: $(â„’.norm(Î”xÎ») / â„’.norm(xÎ»))")
    # end

    return x, â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < sqrt(tol)
end

end # dispatch_doctor

function rrule(::typeof(find_shocks),
                ::Val{:LagrangeNewton},
                initial_guess::Vector{Float64},
                kron_buffer::Vector{Float64},
                kron_buffer2::AbstractMatrix{Float64},
                J::â„’.Diagonal{Bool, Vector{Bool}},
                ğ’â±::AbstractMatrix{Float64},
                ğ’â±Â²áµ‰::AbstractMatrix{Float64},
                shock_independent::Vector{Float64};
                max_iter::Int = 1000,
                tol::Float64 = 1e-13)

    x, matched = find_shocks(Val(:LagrangeNewton),
                            initial_guess,
                            kron_buffer,
                            kron_buffer2,
                            J,
                            ğ’â±,
                            ğ’â±Â²áµ‰,
                            shock_independent,
                            max_iter = max_iter,
                            tol = tol)

    tmp = ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x)

    Î» = tmp' \ x * 2

    fXÎ»p = [reshape(2 * ğ’â±Â²áµ‰' * Î», size(ğ’â±, 2), size(ğ’â±, 2)) - 2 * â„’.I(size(ğ’â±, 2))  tmp'
    -tmp  zeros(size(ğ’â±, 1),size(ğ’â±, 1))]

    â„’.kron!(kron_buffer, x, x)

    xÎ» = â„’.kron(x,Î»)


    âˆ‚shock_independent = similar(shock_independent)

    # âˆ‚ğ’â± = similar(ğ’â±)

    # âˆ‚ğ’â±Â²áµ‰ = similar(ğ’â±Â²áµ‰)

    function find_shocks_pullback(âˆ‚x)
        âˆ‚x = vcat(âˆ‚x[1], zero(Î»))

        S = -fXÎ»p' \ âˆ‚x

        copyto!(âˆ‚shock_independent, S[length(initial_guess)+1:end])
        
        # copyto!(âˆ‚ğ’â±, â„’.kron(S[1:length(initial_guess)], Î») - â„’.kron(x, S[length(initial_guess)+1:end]))
        âˆ‚ğ’â± = S[1:length(initial_guess)] * Î»' - S[length(initial_guess)+1:end] * x'
        
        # copyto!(âˆ‚ğ’â±Â²áµ‰, 2 * â„’.kron(S[1:length(initial_guess)], xÎ») - â„’.kron(kron_buffer, S[length(initial_guess)+1:end]))
        âˆ‚ğ’â±Â²áµ‰ = 2 * S[1:length(initial_guess)] * xÎ»' - S[length(initial_guess)+1:end] * kron_buffer'

        return NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), âˆ‚ğ’â±, âˆ‚ğ’â±Â²áµ‰, âˆ‚shock_independent, NoTangent(), NoTangent()
    end

    return (x, matched), find_shocks_pullback
end


@stable default_mode = "disable" begin

function find_shocks(::Val{:LagrangeNewton},
                    initial_guess::Vector{Float64},
                    kron_buffer::Vector{Float64},
                    kron_bufferÂ²::Vector{Float64},
                    kron_buffer2::AbstractMatrix{Float64},
                    kron_buffer3::AbstractMatrix{Float64},
                    kron_buffer4::AbstractMatrix{Float64},
                    J::â„’.Diagonal{Bool, Vector{Bool}},
                    ğ’â±::AbstractMatrix{Float64},
                    ğ’â±Â²áµ‰::AbstractMatrix{Float64},
                    ğ’â±Â³áµ‰::AbstractMatrix{Float64},
                    shock_independent::Vector{Float64};
                    max_iter::Int = 1000,
                    tol::Float64 = 1e-13) # will fail for higher or lower precision
    x = copy(initial_guess)

    Î» = zeros(size(ğ’â±, 1))
    
    xÎ» = [  x
            Î»   ]

    Î”xÎ» = copy(xÎ»)

    norm1 = â„’.norm(shock_independent) 

    norm2 = 1.0
    
    Î”norm = 1e12

    xÌ‚ = copy(shock_independent)

    xÌ„ = zeros(size(ğ’â±,2))

    âˆ‚x = zero(ğ’â±)

    âˆ‚xÌ‚ = zero(ğ’â±)
    
    fxÎ» = zeros(length(xÎ»))
    
    fxÎ»p = zeros(length(xÎ»), length(xÎ»))

    tmp = zeros(size(ğ’â±, 2) * size(ğ’â±, 2))

    tmp2 = zeros(size(ğ’â±, 1),size(ğ’â±, 2) * size(ğ’â±, 2))

    II = sparse(â„’.I(length(x)^2))

    lI = -2 * vec(â„’.I(size(ğ’â±, 2)))
    
    @inbounds for i in 1:max_iter
        â„’.kron!(kron_buffer2, J, x)
        â„’.kron!(kron_buffer3, J, kron_buffer)

        copy!(âˆ‚x, ğ’â±)
        â„’.mul!(âˆ‚x, ğ’â±Â²áµ‰, kron_buffer2, 2, 1)

        â„’.mul!(âˆ‚x, ğ’â±Â³áµ‰, kron_buffer3, 3, 1)

        â„’.mul!(xÌ„, âˆ‚x', Î»)
        
        â„’.axpy!(-2, x, xÌ„)

        copyto!(fxÎ», 1, xÌ„, 1, size(ğ’â±,2))
        copyto!(fxÎ», size(ğ’â±,2) + 1, xÌ‚, 1, size(shock_independent,1))
        # fXÎ» = [(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
                # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))]
        
        x_kron_II!(kron_buffer4, x)
        # â„’.kron!(kron_buffer4, II, x)
        â„’.mul!(tmp2, ğ’â±Â³áµ‰, kron_buffer4)
        â„’.mul!(tmp, tmp2', Î»)
        â„’.mul!(tmp, ğ’â±Â²áµ‰', Î», 2, 6)
        â„’.axpy!(1,lI,tmp)

        fxÎ»p[1:size(ğ’â±, 2), 1:size(ğ’â±, 2)] = tmp
        
        fxÎ»p[1:size(ğ’â±, 2), size(ğ’â±, 2)+1:end] = âˆ‚x'

        â„’.rmul!(âˆ‚x, -1)
        fxÎ»p[size(ğ’â±, 2)+1:end, 1:size(ğ’â±, 2)] = âˆ‚x
        # fXÎ»p = [reshape((2 * ğ’â±Â²áµ‰ + 6 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(â„’.I(length(x)),x)))' * Î», size(ğ’â±, 2), size(ğ’â±, 2)) - 2*â„’.I(size(ğ’â±, 2))  (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))'
        #         -(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))  zeros(size(ğ’â±, 1),size(ğ’â±, 1))]
        
        try
            fÌ‚xÎ»p = â„’.factorize(fxÎ»p)
            â„’.ldiv!(Î”xÎ», fÌ‚xÎ»p, fxÎ»)
        catch
            # â„’.svd(fxÎ»p)
            # println("factorization fails")
            return x, false
        end
        
        if !all(isfinite,Î”xÎ») break end
        
        â„’.axpy!(-1, Î”xÎ», xÎ»)
        # xÎ» -= Î”xÎ»
    
        # x = xÎ»[1:size(ğ’â±, 2)]
        copyto!(x, 1, xÎ», 1, size(ğ’â±,2))

        # Î» = xÎ»[size(ğ’â±, 2)+1:end]
        copyto!(Î», 1, xÎ», size(ğ’â±,2) + 1, length(Î»))

        â„’.kron!(kron_buffer, x, x)

        â„’.kron!(kron_bufferÂ², x, kron_buffer)

        â„’.mul!(xÌ‚, ğ’â±, x)

        â„’.mul!(xÌ‚, ğ’â±Â²áµ‰, kron_buffer, 1, 1)

        â„’.mul!(xÌ‚, ğ’â±Â³áµ‰, kron_bufferÂ², 1, 1)

        norm2 = â„’.norm(xÌ‚)

        â„’.axpby!(1, shock_independent, -1, xÌ‚)

        if â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < sqrt(tol)
            # println("LagrangeNewton: $i, Tol: $(â„’.norm(Î”xÎ») / â„’.norm(xÎ»)) reached, x: $x")
            break
        end

        # if i > 500 && â„’.norm(Î”xÎ») > 1e-11 && â„’.norm(Î”xÎ») > Î”norm
        #     # println(â„’.norm(Î”xÎ»))
        #     # println(â„’.norm(xÌ‚) / max(norm1,norm2))
        #     # println("LagrangeNewton: $i, Norm increase")
        #     return x, false
        # end
        # if i == max_iter
        #     println("LagrangeNewton: $i, Max iter reached")
        #     # println(â„’.norm(Î”xÎ»))
        # end
    end

    # Î» = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), kron_buffer))' \ x * 2
    # println("LagrangeNewton: $(â„’.norm([(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
    # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))]))")

    # println(â„’.norm(x))
    # println(x)
    # println(Î»)
    # println([(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) - ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
    # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))])
    # println(fxÎ»p)
    # println(reshape(tmp, size(ğ’â±, 2), size(ğ’â±, 2)) - 2*â„’.I(size(ğ’â±, 2)))
    # println([reshape((2 * ğ’â±Â²áµ‰ - 2 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(â„’.I(length(x)),x)))' * Î», size(ğ’â±, 2), size(ğ’â±, 2)) - 2*â„’.I(size(ğ’â±, 2))  (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) - ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))'
    #         -(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) - ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))  zeros(size(ğ’â±, 1),size(ğ’â±, 1))])
    # println(fxÎ»p)
    # println("Norm: $(â„’.norm(xÌ‚) / max(norm1,norm2))")
    # println(â„’.norm(Î”xÎ»))
    # println(â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < tol)

    # if !(â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < sqrt(tol))
    #     println("Find shocks failed. Norm 1: $(â„’.norm(xÌ‚) / max(norm1,norm2)); Norm 2: $(â„’.norm(Î”xÎ») / â„’.norm(xÎ»))")
    # end

    return x, â„’.norm(xÌ‚) / max(norm1,norm2) < tol && â„’.norm(Î”xÎ») / â„’.norm(xÎ») < sqrt(tol)
end


end # dispatch_doctor


function rrule(::typeof(find_shocks),
                ::Val{:LagrangeNewton},
                initial_guess::Vector{Float64},
                kron_buffer::Vector{Float64},
                kron_bufferÂ²::Vector{Float64},
                kron_buffer2::AbstractMatrix{Float64},
                kron_buffer3::AbstractMatrix{Float64},
                kron_buffer4::AbstractMatrix{Float64},
                J::â„’.Diagonal{Bool, Vector{Bool}},
                ğ’â±::AbstractMatrix{Float64},
                ğ’â±Â²áµ‰::AbstractMatrix{Float64},
                ğ’â±Â³áµ‰::AbstractMatrix{Float64},
                shock_independent::Vector{Float64};
                max_iter::Int = 1000,
                tol::Float64 = 1e-13)

    x, matched = find_shocks(Val(:LagrangeNewton),
                            initial_guess,
                            kron_buffer,
                            kron_bufferÂ²,
                            kron_buffer2,
                            kron_buffer3,
                            kron_buffer4,
                            J,
                            ğ’â±,
                            ğ’â±Â²áµ‰,
                            ğ’â±Â³áµ‰,
                            shock_independent,
                            max_iter = max_iter,
                            tol = tol)

    â„’.kron!(kron_buffer, x, x)

    â„’.kron!(kron_bufferÂ², x, kron_buffer)

    tmp = ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), kron_buffer)

    Î» = tmp' \ x * 2

    fXÎ»p = [reshape((2 * ğ’â±Â²áµ‰ + 6 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(â„’.I(length(x)),x)))' * Î», size(ğ’â±, 2), size(ğ’â±, 2)) - 2 * â„’.I(size(ğ’â±, 2))  tmp'
    -tmp  zeros(size(ğ’â±, 1),size(ğ’â±, 1))]

    xÎ» = â„’.kron(x,Î»)

    xxÎ» = â„’.kron(x,xÎ»)

    function find_shocks_pullback(âˆ‚x)
        âˆ‚x = vcat(âˆ‚x[1], zero(Î»))

        S = -fXÎ»p' \ âˆ‚x

        âˆ‚shock_independent = S[length(initial_guess)+1:end]
        
        âˆ‚ğ’â± = â„’.kron(S[1:length(initial_guess)], Î») - â„’.kron(x, S[length(initial_guess)+1:end])

        âˆ‚ğ’â±Â²áµ‰ = 2 * â„’.kron(S[1:length(initial_guess)], xÎ») - â„’.kron(kron_buffer, S[length(initial_guess)+1:end])
        
        âˆ‚ğ’â±Â³áµ‰ = 3 * â„’.kron(S[1:length(initial_guess)], xxÎ») - â„’.kron(kron_bufferÂ²,S[length(initial_guess)+1:end])

        return NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(), NoTangent(),  âˆ‚ğ’â±, âˆ‚ğ’â±Â²áµ‰, âˆ‚ğ’â±Â³áµ‰, âˆ‚shock_independent, NoTangent(), NoTangent()
    end

    return (x, matched), find_shocks_pullback
end



# @stable default_mode = "disable" begin

# function find_shocks(::Val{:SLSQP},
#                     initial_guess::Vector{Float64},
#                     kron_buffer::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     max_iter::Int = 500,
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision
#     function objective_optim_fun(X::Vector{S}, grad::Vector{S}) where S
#         if length(grad) > 0
#             copy!(grad, X)

#             â„’.rmul!(grad, 2)
#             # grad .= 2 .* X
#         end
        
#         sum(abs2, X)
#     end

#     function constraint_optim(res::Vector{S}, x::Vector{S}, jac::Matrix{S}) where S <: Float64
#         if length(jac) > 0
#             â„’.kron!(kron_buffer2, J, x)

#             copy!(jac', ğ’â±)

#             â„’.mul!(jac', ğ’â±Â²áµ‰, kron_buffer2, -2, -1)
#             # jac .= -(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))'
#         end

#         â„’.kron!(kron_buffer, x, x)

#         â„’.mul!(res, ğ’â±, x)

#         â„’.mul!(res, ğ’â±Â²áµ‰, kron_buffer, 1, 1)

#         â„’.axpby!(1, shock_independent, -1, res)
#         # res .= shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * â„’.kron(X,X)
#     end
    
#     opt = NLopt.Opt(NLopt.:LD_SLSQP, size(ğ’â±,2))
                    
#     opt.min_objective = objective_optim_fun

#     # opt.xtol_abs = eps()
#     # opt.ftol_abs = eps()
#     opt.maxeval = max_iter

#     NLopt.equality_constraint!(opt, constraint_optim, fill(eps(),size(ğ’â±,1)))

#     (minf,x,ret) = try 
#         NLopt.optimize(opt, initial_guess)
#     catch
#         return initial_guess, false
#     end

#     â„’.kron!(kron_buffer, x, x)

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron_buffer

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     solved = ret âˆˆ Symbol.([
#         # NLopt.MAXEVAL_REACHED,
#         NLopt.SUCCESS,
#         NLopt.STOPVAL_REACHED,
#         NLopt.FTOL_REACHED,
#         NLopt.XTOL_REACHED,
#         NLopt.ROUNDOFF_LIMITED,
#     ])

#     # println(â„’.norm(x))
#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol && solved
# end



# function find_shocks(::Val{:SLSQP},
#                     initial_guess::Vector{Float64},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     kron_buffer4::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     max_iter::Int = 500,
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision
#     function objective_optim_fun(X::Vector{S}, grad::Vector{S}) where S
#         if length(grad) > 0
#             copy!(grad, X)

#             â„’.rmul!(grad, 2)
#             # grad .= 2 .* X
#         end
        
#         sum(abs2, X)
#     end

#     function constraint_optim(res::Vector{S}, x::Vector{S}, jac::Matrix{S}) where S <: Float64
#         â„’.kron!(kron_buffer, x, x)

#         â„’.kron!(kron_bufferÂ², x, kron_buffer)

#         if length(jac) > 0
#             â„’.kron!(kron_buffer2, J, x)

#             â„’.kron!(kron_buffer3, J, kron_buffer)

#             copy!(jac', ğ’â±)

#             â„’.mul!(jac', ğ’â±Â²áµ‰, kron_buffer2, 2, 1)

#             â„’.mul!(jac', ğ’â±Â³áµ‰, kron_buffer3, -3, -1)
#             # jac .= -(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(J, x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(J, â„’.kron(x,x)))'
#         end

#         â„’.mul!(res, ğ’â±, x)

#         â„’.mul!(res, ğ’â±Â²áµ‰, kron_buffer, 1, 1)

#         â„’.mul!(res, ğ’â±Â³áµ‰, kron_bufferÂ², 1, 1)

#         â„’.axpby!(1, shock_independent, -1, res)
#         # res .= shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * â„’.kron!(kron_buffer, x, x) - ğ’â±Â³áµ‰ * â„’.kron!(kron_bufferÂ², x, kron_buffer)
#     end
    
#     opt = NLopt.Opt(NLopt.:LD_SLSQP, size(ğ’â±,2))
                    
#     opt.min_objective = objective_optim_fun

#     # opt.xtol_abs = eps()
#     # opt.ftol_abs = eps()
#     # opt.constrtol_abs = eps() # doesn't work
#     # opt.xtol_rel = eps()
#     # opt.ftol_rel = eps()
#     opt.maxeval = max_iter

#     NLopt.equality_constraint!(opt, constraint_optim, fill(eps(),size(ğ’â±,1)))

#     (minf,x,ret) = try 
#         NLopt.optimize(opt, initial_guess)
#     catch
#         return initial_guess, false
#     end

#     # println("SLSQP - retcode: $ret, nevals: $(opt.numevals)")

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     solved = ret âˆˆ Symbol.([
#         # NLopt.MAXEVAL_REACHED,
#         NLopt.SUCCESS,
#         NLopt.STOPVAL_REACHED,
#         NLopt.FTOL_REACHED,
#         NLopt.XTOL_REACHED,
#         NLopt.ROUNDOFF_LIMITED,
#     ])

#     # println(â„’.norm(x))
#     # Î» = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), kron_buffer))' \ x * 2
#     # println("SLSQP - $ret: $(â„’.norm([(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
#     # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))]))")
#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol && solved
# end






# function find_shocks(::Val{:COBYLA},
#                     initial_guess::Vector{Float64},
#                     kron_buffer::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     max_iter::Int = 10000,
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision
#     function objective_optim_fun(X::Vector{S}, grad::Vector{S}) where S
#         sum(abs2, X)
#     end

#     function constraint_optim(res::Vector{S}, x::Vector{S}, jac::Matrix{S}) where S <: Float64
#         â„’.kron!(kron_buffer, x, x)

#         â„’.mul!(res, ğ’â±, x)

#         â„’.mul!(res, ğ’â±Â²áµ‰, kron_buffer, 1, 1)

#         â„’.axpby!(1, shock_independent, -1, res)
#         # res .= shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * â„’.kron(X,X)
#     end

#     opt = NLopt.Opt(NLopt.:LN_COBYLA, size(ğ’â±,2))
                    
#     opt.min_objective = objective_optim_fun

#     # opt.xtol_abs = eps()
#     # opt.ftol_abs = eps()
#     # opt.xtol_rel = eps()
#     # opt.ftol_rel = eps()
#     # opt.constrtol_abs = eps() # doesn't work
#     opt.maxeval = max_iter

#     NLopt.equality_constraint!(opt, constraint_optim, fill(eps(),size(ğ’â±,1)))

#     (minf,x,ret) = NLopt.optimize(opt, initial_guess)

#     # println("COBYLA - retcode: $ret, nevals: $(opt.numevals)")

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x)

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     solved = ret âˆˆ Symbol.([
#         NLopt.SUCCESS,
#         NLopt.STOPVAL_REACHED,
#         NLopt.FTOL_REACHED,
#         NLopt.XTOL_REACHED,
#         NLopt.ROUNDOFF_LIMITED,
#     ])

#     # println("COBYLA: $(opt.numevals)")

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol && solved
# end



# function find_shocks(::Val{:COBYLA},
#                     initial_guess::Vector{Float64},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     kron_buffer4::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     max_iter::Int = 10000,
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision
#     function objective_optim_fun(X::Vector{S}, grad::Vector{S}) where S
#         sum(abs2, X)
#     end

#     function constraint_optim(res::Vector{S}, x::Vector{S}, jac::Matrix{S}) where S <: Float64
#         â„’.kron!(kron_buffer, x, x)

#         â„’.kron!(kron_bufferÂ², x, kron_buffer)

#         â„’.mul!(res, ğ’â±, x)

#         â„’.mul!(res, ğ’â±Â²áµ‰, kron_buffer, 1, 1)

#         â„’.mul!(res, ğ’â±Â³áµ‰, kron_bufferÂ², 1, 1)

#         â„’.axpby!(1, shock_independent, -1, res)
#         # res .= shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * â„’.kron(X,X) - ğ’â±Â³áµ‰ * â„’.kron(X, â„’.kron(X,X))
#     end

#     opt = NLopt.Opt(NLopt.:LN_COBYLA, size(ğ’â±,2))
                    
#     opt.min_objective = objective_optim_fun

#     # opt.xtol_abs = eps()
#     # opt.ftol_abs = eps()
#     # opt.xtol_rel = eps()
#     # opt.ftol_rel = eps()
#     # opt.constrtol_abs = eps() # doesn't work
#     opt.maxeval = max_iter

#     NLopt.equality_constraint!(opt, constraint_optim, fill(eps(),size(ğ’â±,1)))

#     (minf,x,ret) = NLopt.optimize(opt, initial_guess)

#     # println("COBYLA - retcode: $ret, nevals: $(opt.numevals)")

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     solved = ret âˆˆ Symbol.([
#         NLopt.SUCCESS,
#         NLopt.STOPVAL_REACHED,
#         NLopt.FTOL_REACHED,
#         NLopt.XTOL_REACHED,
#         NLopt.ROUNDOFF_LIMITED,
#     ])
#     # println(â„’.norm(x))
#     # println(x)
#     # Î» = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), kron_buffer))' \ x * 2
#     # println("COBYLA: $(â„’.norm([(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
#     # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))]))")
#     # println("COBYLA: $(opt.numevals)")
#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")

#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol && solved
# end

# end # dispatch_doctor





# function find_shocks(::Val{:MadNLP},
#                     initial_guess::Vector{Float64},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     kron_buffer4::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     max_iter::Int = 500,
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision
#     model = JuMP.Model(MadNLP.Optimizer)

#     JuMP.set_silent(model)

#     JuMP.set_optimizer_attribute(model, "tol", tol)

#     JuMP.@variable(model, x[1:length(initial_guess)])

#     JuMP.set_start_value.(x, initial_guess)

#     JuMP.@objective(model, Min, sum(abs2,x))

#     JuMP.@constraint(model, ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)) .== shock_independent)

#     JuMP.optimize!(model)

#     x = JuMP.value.(x)

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println(â„’.norm(y - shock_independent) / max(norm1,norm2))
#     # Î» = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), kron_buffer))' \ x * 2
#     # println("SLSQP - $ret: $(â„’.norm([(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
#     # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))]))")
#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end




# function find_shocks(::Val{:Ipopt},
#                     initial_guess::Vector{Float64},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     kron_buffer4::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     max_iter::Int = 500,
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision
#     model = JuMP.Model(Ipopt.Optimizer)

#     JuMP.set_silent(model)

#     JuMP.set_optimizer_attribute(model, "tol", tol)

#     JuMP.@variable(model, x[1:length(initial_guess)])

#     JuMP.set_start_value.(x, initial_guess)

#     JuMP.@objective(model, Min, sum(abs2,x))

#     JuMP.@constraint(model, ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)) .== shock_independent)

#     JuMP.optimize!(model)

#     x = JuMP.value.(x)

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println(â„’.norm(y - shock_independent) / max(norm1,norm2))
#     # Î» = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), kron_buffer))' \ x * 2
#     # println("SLSQP - $ret: $(â„’.norm([(ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) + 3 * ğ’â±Â³áµ‰ * â„’.kron(â„’.I(length(x)), â„’.kron(x, x)))' * Î» - 2 * x
#     # shock_independent - (ğ’â± * x + ğ’â±Â²áµ‰ * â„’.kron(x,x) + ğ’â±Â³áµ‰ * â„’.kron(x, â„’.kron(x, x)))]))")
#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end


# function find_shocks(::Val{:newton},
#     kron_buffer::Vector{Float64},
#     kron_buffer2::AbstractMatrix{Float64},
#     J::â„’.Diagonal{Bool, Vector{Bool}},
#     ğ’â±::AbstractMatrix{Float64},
#     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#     shock_independent::Vector{Float64};
#     tol::Float64 = 1e-13) # will fail for higher or lower precision

#     nExo = Int(sqrt(length(kron_buffer)))

#     x = zeros(nExo)

#     xÌ‚ = zeros(size(ğ’â±Â²áµ‰,1))

#     xÌ‚ = zeros(size(ğ’â±Â²áµ‰,1))

#     xÌ„ = zeros(size(ğ’â±Â²áµ‰,1))

#     Î”x = zeros(nExo)

#     âˆ‚x = zero(ğ’â±)

#     JÌ‚ = â„’.I(nExo)*2

#     max_iter = 1000

# 	norm1 = 1

# 	norm2 = â„’.norm(shock_independent)

#     for i in 1:max_iter
#         â„’.kron!(kron_buffer, x, x)
#         â„’.kron!(kron_buffer2, JÌ‚, x)
        
#         â„’.mul!(âˆ‚x, ğ’â±Â²áµ‰, kron_buffer2)
#         â„’.axpy!(1, ğ’â±, âˆ‚x)
#         # âˆ‚x = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(nExo), x))

#         âˆ‚xÌ‚ = try 
#             â„’.factorize(âˆ‚x)
#         catch
#             return x, false
#         end 

#         â„’.mul!(xÌ‚, ğ’â±Â²áµ‰, kron_buffer)
#         â„’.mul!(xÌ„, ğ’â±, x)
#         â„’.axpy!(1, xÌ„, xÌ‚)
# 				norm1 = â„’.norm(xÌ‚)
#         â„’.axpby!(1, shock_independent, -1, xÌ‚)
#         try 
#             â„’.ldiv!(Î”x, âˆ‚xÌ‚, xÌ‚)
#         catch
#             return x, false
#         end
#         # Î”x = âˆ‚xÌ‚ \ (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron_buffer)
#         # println(â„’.norm(Î”x))
#         if i > 6 && (â„’.norm(xÌ‚) / max(norm1,norm2) < tol)
#             # println(i)
#             break
#         end
        
#         â„’.axpy!(1, Î”x, x)
#         # x += Î”x

#         if !all(isfinite.(x))
#             return x, false
#         end
#     end

#     return x, â„’.norm(xÌ‚) / max(norm1,norm2) < tol
# end



# function find_shocks(::Val{:newton},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = 1e-13) # will fail for higher or lower precision

#     nExo = Int(sqrt(length(kron_buffer)))

#     x = zeros(nExo)

#     xÌ‚ = zeros(size(ğ’â±Â²áµ‰,1))

#     xÌ‚ = zeros(size(ğ’â±Â²áµ‰,1))

#     xÌ„ = zeros(size(ğ’â±Â²áµ‰,1))

#     Î”x = zeros(nExo)

#     âˆ‚x = zero(ğ’â±)

#     JÌ‚ = â„’.I(nExo)*2

#     max_iter = 1000

#     norm1 = 1

# 	norm2 = â„’.norm(shock_independent)

#     for i in 1:max_iter
#         â„’.kron!(kron_buffer, x, x)
#         â„’.kron!(kron_bufferÂ², x, kron_buffer)
#         â„’.kron!(kron_buffer2, JÌ‚, x)
#         â„’.kron!(kron_buffer3, JÌ‚, kron_buffer)
        
#         â„’.mul!(âˆ‚x, ğ’â±Â²áµ‰, kron_buffer2)
#         â„’.mul!(âˆ‚x, ğ’â±Â³áµ‰, kron_buffer3, 1 ,1)
#         â„’.axpy!(1, ğ’â±, âˆ‚x)
#         # âˆ‚x = (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(nExo), x) + ğ’â±Â³áµ‰ * â„’.kron(â„’.I(nExo), â„’.kron(x,x)))

#         âˆ‚xÌ‚ = try 
#             â„’.factorize(âˆ‚x)
#         catch
#             return x, false
#         end 
							
#         â„’.mul!(xÌ‚, ğ’â±Â²áµ‰, kron_buffer)
#         â„’.mul!(xÌ‚, ğ’â±Â³áµ‰, kron_bufferÂ², 1, 1)
#         â„’.mul!(xÌ„, ğ’â±, x)
#         â„’.axpy!(1, xÌ„, xÌ‚)
# 				norm1 = â„’.norm(xÌ‚)
#         â„’.axpby!(1, shock_independent, -1, xÌ‚)
#         try 
#             â„’.ldiv!(Î”x, âˆ‚xÌ‚, xÌ‚)
#         catch
#             return x, false
#         end
#         # Î”x = âˆ‚xÌ‚ \ (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron_buffer)
#         # println(â„’.norm(Î”x))
#         if i > 6 && (â„’.norm(xÌ‚) / max(norm1,norm2)) < tol
#             # println("Iters: $i Norm: $(â„’.norm(xÌ‚) / max(norm1,norm2))")
#             break
#         end
        
#         â„’.axpy!(1, Î”x, x)
#         # x += Î”x

#         if !all(isfinite.(x))
#             return x, false
#         end
#     end

#     # println("Iters: $max_iter Norm: $(â„’.norm(xÌ‚) / max(norm1,norm2))")
#     return x, â„’.norm(xÌ‚) / max(norm1,norm2) < tol
# end



# function find_shocks(::Val{:LBFGS},
#                     kron_buffer::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = 1e-15) # will fail for higher or lower precision

#     function optim_fun(x::Vector{S}, grad::Vector{S}) where S <: Float64
#         if length(grad) > 0
#             grad .= - (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))' * (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)) / sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)))
#         end

#         return sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)))
#     end
    

#     opt = NLopt.Opt(NLopt.:LD_LBFGS, size(ğ’â±,2))
                    
#     opt.min_objective = optim_fun

#     opt.xtol_abs = eps()
#     opt.ftol_abs = eps()
#     opt.maxeval = 10000

#     (minf,x,ret) = NLopt.optimize(opt, zeros(size(ğ’â±,2)))

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron(x,x)

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end




# function find_shocks(::Val{:LBFGS},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = eps()) # will fail for higher or lower precision

#     function optim_fun(x::Vector{S}, grad::Vector{S}) where S <: Float64
#         if length(grad) > 0
#             grad .= - (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) - ğ’â±Â³áµ‰ * kron(â„’.I(length(x)),kron(x,x)))' * (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))) / sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))))
#         end

#         return sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))))
#     end

#     opt = NLopt.Opt(NLopt.:LD_LBFGS, size(ğ’â±,2))
                    
#     opt.min_objective = optim_fun

#     # opt.xtol_abs = eps()
#     # opt.ftol_abs = eps()
#     opt.maxeval = 10000

#     (minf,x,ret) = NLopt.optimize(opt, zeros(size(ğ’â±,2)))

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron(x,x) + ğ’â±Â³áµ‰ * kron(x,kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end



# function find_shocks(::Val{:LBFGSjl},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = 1e-15) # will fail for higher or lower precision

#     function f(X)
#         sqrt(sum(abs2, shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * kron(X,X) - ğ’â±Â³áµ‰ * kron(X,kron(X,X))))
#     end

#     function g!(G, x)
#         G .= - (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) - ğ’â±Â³áµ‰ * kron(â„’.I(length(x)),kron(x,x)))' * (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))) / sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))))
#     end

#     sol = Optim.optimize(f,g!,
#         # X -> sqrt(sum(abs2, shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * kron(X,X) - ğ’â±Â³áµ‰ * kron(X,kron(X,X)))),
#                         zeros(size(ğ’â±,2)), 
#                         Optim.LBFGS(linesearch = LineSearches.BackTracking(order = 3)))#; 
#                         # autodiff = :forward)

#     x = sol.minimizer

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron(x,x) + ğ’â±Â³áµ‰ * kron(x,kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end



# function find_shocks(::Val{:LBFGSjl},
#                     kron_buffer::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = 1e-15) # will fail for higher or lower precision

#     function f(X)
#         sqrt(sum(abs2, shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * kron(X,X)))
#     end

#     function g!(G, x)
#         G .= - (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))' * (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)) / sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)))
#     end

#     sol = Optim.optimize(f,g!,
#     # X -> sqrt(sum(abs2, shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * kron(X,X))),
#                         zeros(size(ğ’â±,2)), 
#                         Optim.LBFGS(linesearch = LineSearches.BackTracking(order = 3)))#; 
#                         # autodiff = :forward)

#     x = sol.minimizer

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron(x,x)

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end


# function find_shocks(::Val{:speedmapping},
#                     kron_buffer::Vector{Float64},
#                     kron_bufferÂ²::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     kron_buffer3::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     ğ’â±Â³áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = 1e-15) # will fail for higher or lower precision

#     function f(X)
#         sqrt(sum(abs2, shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * kron(X,X) - ğ’â±Â³áµ‰ * kron(X,kron(X,X))))
#     end

#     function g!(G, x)
#         G .= - (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x) - ğ’â±Â³áµ‰ * kron(â„’.I(length(x)),kron(x,x)))' * (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))) / sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x) - ğ’â±Â³áµ‰ * kron(x,kron(x,x))))
#     end

#     sol = speedmapping(zeros(size(ğ’â±,2)), f = f, g! = g!, tol = tol, maps_limit = 10000, stabilize = false)
# println(sol)
#     x = sol.minimizer

#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron(x,x) + ğ’â±Â³áµ‰ * kron(x,kron(x,x))

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end




# function find_shocks(::Val{:speedmapping},
#                     kron_buffer::Vector{Float64},
#                     kron_buffer2::AbstractMatrix{Float64},
#                     J::â„’.Diagonal{Bool, Vector{Bool}},
#                     ğ’â±::AbstractMatrix{Float64},
#                     ğ’â±Â²áµ‰::AbstractMatrix{Float64},
#                     shock_independent::Vector{Float64};
#                     tol::Float64 = 1e-15) # will fail for higher or lower precision
#     function f(X)
#         sqrt(sum(abs2, shock_independent - ğ’â± * X - ğ’â±Â²áµ‰ * kron(X,X)))
#     end

#     function g!(G, x)
#         G .= - (ğ’â± + 2 * ğ’â±Â²áµ‰ * â„’.kron(â„’.I(length(x)), x))' * (shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)) / sqrt(sum(abs2, shock_independent - ğ’â± * x - ğ’â±Â²áµ‰ * kron(x,x)))
#     end

#     sol = speedmapping(zeros(size(ğ’â±,2)), f = f, g! = g!, tol = tol, maps_limit = 10000, stabilize = false)

#     x = sol.minimizer
    
#     y = ğ’â± * x + ğ’â±Â²áµ‰ * kron(x,x)

#     norm1 = â„’.norm(y)

# 	norm2 = â„’.norm(shock_independent)

#     # println("Norm: $(â„’.norm(y - shock_independent) / max(norm1,norm2))")
#     return x, â„’.norm(y - shock_independent) / max(norm1,norm2) < tol
# end