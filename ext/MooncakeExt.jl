"""
    MooncakeExt

Extension module for MacroModelling.jl that provides Mooncake.jl automatic differentiation 
support with native rrule!! implementations.

This extension enables efficient reverse-mode AD for DSGE model estimation workflows, 
particularly for computing gradients of log-likelihood functions with respect to model 
parameters.

The key function with native Mooncake rules is:
- `get_loglikelihood` - Main log-likelihood function for all algorithm/filter combinations

The rrule!! computes gradients analytically by chaining the gradients through the internal
computation pipeline:
1. NSSS (steady state) â†’ Jacobian â†’ First-order solution â†’ Filter log-likelihood
2. Each step has analytical gradient rules that are chained together

This approach:
- Does NOT use Zygote, FiniteDifferences, or any other AD package
- Implements the same analytical gradient logic as the existing ChainRulesCore rrules
- Ensures Mooncake uses the custom rule rather than differentiating through internals

## Usage

```julia
using MacroModelling
using Mooncake
using DifferentiationInterface

# Define your model
@model RBC begin
    # ... equations ...
end

@parameters RBC begin
    # ... parameters ...
end

# Compute gradient with Mooncake
backend = DifferentiationInterface.AutoMooncake(; config=nothing)
grad = DifferentiationInterface.gradient(
    p -> get_loglikelihood(RBC, data, p),
    backend,
    parameter_values
)
```
"""
module MooncakeExt

using MacroModelling
import Mooncake
import Mooncake: @is_primitive, MinimalCtx, DefaultCtx, zero_tangent, tangent_type

# Import types and functions needed for rule definitions
import MacroModelling: â„³, timings, CalculationOptions, merge_calculation_options, Tolerances
import MacroModelling: get_loglikelihood, get_NSSS_and_parameters, calculate_jacobian
import MacroModelling: calculate_first_order_solution, solve_lyapunov_equation, solve_sylvester_equation
import MacroModelling: calculate_kalman_filter_loglikelihood, run_kalman_iterations
import MacroModelling: calculate_inversion_filter_loglikelihood
import MacroModelling: get_initial_covariance
import MacroModelling: check_bounds, normalize_filtering_options, get_and_check_observables
import MacroModelling: DEFAULT_ALGORITHM, DEFAULT_FILTER_SELECTOR
import MacroModelling: DEFAULT_WARMUP_ITERATIONS, DEFAULT_PRESAMPLE_PERIODS
import MacroModelling: DEFAULT_QME_ALGORITHM, DEFAULT_LYAPUNOV_ALGORITHM, DEFAULT_SYLVESTER_SELECTOR
import MacroModelling: DEFAULT_VERBOSE, DEFAULT_SYLVESTER_THRESHOLD, DEFAULT_LARGE_SYLVESTER_ALGORITHM, DEFAULT_SYLVESTER_ALGORITHM
import MacroModelling: replace_indices, rrule

import AxisKeys: KeyedArray, rekey, axiskeys
import LinearAlgebra as â„’
import RecursiveFactorization as RF
import SparseArrays: sparse, nnz, SparseMatrixCSC
import ChainRulesCore: NoTangent
import Accessors: @ignore_derivatives

# ================================================================================================
# get_loglikelihood - Main entry point for likelihood computation
# Native rrule!! that handles all algorithm/filter combinations with analytical gradients
# ================================================================================================

# Mark get_loglikelihood as a primitive for Mooncake - this prevents it from 
# trying to differentiate through the internals
@is_primitive MinimalCtx Tuple{typeof(get_loglikelihood), â„³, KeyedArray{Float64}, Vector{Float64}}

"""
    compute_analytical_gradient_first_order_kalman(...)

Compute the gradient of log-likelihood w.r.t. parameters for first_order algorithm with kalman filter.
This chains together the analytical gradients from:
1. get_NSSS_and_parameters
2. calculate_jacobian
3. calculate_first_order_solution
4. solve_lyapunov_equation (for initial covariance)
5. run_kalman_iterations
"""
function compute_analytical_gradient_first_order_kalman(
    ğ“‚::â„³,
    parameter_values::Vector{Float64},
    data_in_deviations::Matrix{Float64},
    observables::Vector{Symbol},
    obs_indices::Vector{Int},
    presample_periods::Int,
    initial_covariance::Symbol,
    opts::CalculationOptions
)
    # Forward pass with caching for backward pass
    # Step 1: Get NSSS and parameters with its rrule
    (SS_and_pars, (solution_error, iters)), nsss_pullback = rrule(
        get_NSSS_and_parameters, ğ“‚, parameter_values; opts = opts
    )
    
    if solution_error > opts.tol.NSSS_acceptance_tol
        return zeros(length(parameter_values))
    end
    
    # Step 2: Calculate Jacobian with its rrule
    âˆ‡â‚, jacobian_pullback = rrule(calculate_jacobian, parameter_values, SS_and_pars, ğ“‚)
    
    # Step 3: Calculate first-order solution with its rrule
    TT = ğ“‚.timings
    (ğ’â‚, qme_sol, solved), first_order_pullback = rrule(
        calculate_first_order_solution, âˆ‡â‚; 
        T = TT, 
        opts = opts,
        initial_guess = ğ“‚.solution.perturbation.qme_solution
    )
    
    if !solved
        return zeros(length(parameter_values))
    end
    
    # Step 4: Setup Kalman filter matrices
    observables_and_states = sort(union(TT.past_not_future_and_mixed_idx, 
                                       convert(Vector{Int}, indexin(observables, sort(union(TT.aux, TT.var, TT.exo_present))))))
    
    obs_idx = convert(Vector{Int}, indexin(observables, sort(union(TT.aux, TT.var, TT.exo_present))))
    
    A = ğ’â‚[observables_and_states, 1:TT.nPast_not_future_and_mixed] * 
        â„’.diagm(ones(Float64, length(observables_and_states)))[indexin(TT.past_not_future_and_mixed_idx, observables_and_states), :]
    B = ğ’â‚[observables_and_states, TT.nPast_not_future_and_mixed+1:end]
    C = â„’.diagm(ones(length(observables_and_states)))[indexin(sort(obs_idx), observables_and_states), :]
    ğ = B * B'
    
    # Step 5: Get initial covariance with rrule for solve_lyapunov_equation
    (P, lyap_solved), lyapunov_pullback = rrule(
        solve_lyapunov_equation, A, ğ;
        lyapunov_algorithm = opts.lyapunov_algorithm,
        tol = opts.tol.lyapunov_tol,
        acceptance_tol = opts.tol.lyapunov_acceptance_tol,
        verbose = opts.verbose
    )
    
    if !lyap_solved
        return zeros(length(parameter_values))
    end
    
    # Step 6: Run Kalman iterations with rrule
    llh, kalman_pullback = rrule(
        run_kalman_iterations, A, ğ, C, P, data_in_deviations;
        presample_periods = presample_periods,
        verbose = opts.verbose
    )
    
    # Backward pass - chain the gradients
    âˆ‚llh = 1.0  # Derivative of output w.r.t. itself
    
    # Kalman pullback: returns NoTangent, âˆ‚A, âˆ‚ğ, NoTangent, âˆ‚P, âˆ‚data_in_deviations, NoTangent
    _, âˆ‚A_kalman, âˆ‚ğ_kalman, _, âˆ‚P_kalman, âˆ‚data_in_deviations, _ = kalman_pullback(âˆ‚llh)
    
    # Lyapunov pullback: âˆ‚P â†’ âˆ‚A, âˆ‚ğ (from covariance)
    _, âˆ‚A_lyap, âˆ‚ğ_lyap, _ = lyapunov_pullback((âˆ‚P_kalman, nothing))
    
    # Combine gradients w.r.t. A and ğ
    âˆ‚A = âˆ‚A_kalman + âˆ‚A_lyap
    âˆ‚ğ = âˆ‚ğ_kalman + âˆ‚ğ_lyap
    
    # Gradient through ğ = B * B' â†’ âˆ‚B = (âˆ‚ğ + âˆ‚ğ') * B
    âˆ‚B = (âˆ‚ğ + âˆ‚ğ') * B
    
    # Gradient w.r.t. ğ’â‚ from A and B
    # A = ğ’â‚[observables_and_states, 1:nâ‚‹] * selection_matrix
    # B = ğ’â‚[observables_and_states, nâ‚‹+1:end]
    âˆ‚ğ’â‚ = zeros(size(ğ’â‚))
    selection = â„’.diagm(ones(Float64, length(observables_and_states)))[indexin(TT.past_not_future_and_mixed_idx, observables_and_states), :]
    âˆ‚ğ’â‚[observables_and_states, 1:TT.nPast_not_future_and_mixed] = âˆ‚A * selection'
    âˆ‚ğ’â‚[observables_and_states, TT.nPast_not_future_and_mixed+1:end] = âˆ‚B
    
    # Gradient w.r.t. SS_and_pars from data_in_deviations
    # data_in_deviations = dt .- SS_and_pars[obs_indices]
    âˆ‚SS_and_pars_data = zeros(length(SS_and_pars))
    âˆ‚SS_and_pars_data[obs_indices] = -sum(âˆ‚data_in_deviations, dims=2)[:]
    
    # First-order solution pullback: âˆ‚ğ’â‚ â†’ âˆ‚âˆ‡â‚
    _, âˆ‚âˆ‡â‚, _ = first_order_pullback((âˆ‚ğ’â‚, nothing, nothing))
    
    # Jacobian pullback: âˆ‚âˆ‡â‚ â†’ âˆ‚parameters, âˆ‚SS_and_pars
    _, âˆ‚parameters_jac, âˆ‚SS_and_pars_jac, _ = jacobian_pullback(âˆ‚âˆ‡â‚)
    
    # NSSS pullback: âˆ‚SS_and_pars â†’ âˆ‚parameters
    âˆ‚SS_and_pars_total = âˆ‚SS_and_pars_jac + âˆ‚SS_and_pars_data
    _, _, âˆ‚parameters_nsss, _ = nsss_pullback((âˆ‚SS_and_pars_total, nothing))
    
    # Total gradient w.r.t. parameters
    âˆ‚parameters = âˆ‚parameters_jac + âˆ‚parameters_nsss
    
    return âˆ‚parameters
end

"""
    compute_analytical_gradient_first_order_inversion(...)

Compute the gradient of log-likelihood w.r.t. parameters for first_order algorithm with inversion filter.
"""
function compute_analytical_gradient_first_order_inversion(
    ğ“‚::â„³,
    parameter_values::Vector{Float64},
    data_in_deviations::Matrix{Float64},
    observables::Union{Vector{Symbol}, Vector{String}},
    obs_indices::Vector{Int},
    presample_periods::Int,
    warmup_iterations::Int,
    filter_algorithm::Symbol,
    opts::CalculationOptions
)
    # Forward pass with caching for backward pass
    # Step 1: Get NSSS and parameters with its rrule
    (SS_and_pars, (solution_error, iters)), nsss_pullback = rrule(
        get_NSSS_and_parameters, ğ“‚, parameter_values; opts = opts
    )
    
    if solution_error > opts.tol.NSSS_acceptance_tol
        return zeros(length(parameter_values))
    end
    
    # Step 2: Calculate Jacobian with its rrule
    âˆ‡â‚, jacobian_pullback = rrule(calculate_jacobian, parameter_values, SS_and_pars, ğ“‚)
    
    # Step 3: Calculate first-order solution with its rrule
    TT = ğ“‚.timings
    (ğ’â‚, qme_sol, solved), first_order_pullback = rrule(
        calculate_first_order_solution, âˆ‡â‚; 
        T = TT, 
        opts = opts,
        initial_guess = ğ“‚.solution.perturbation.qme_solution
    )
    
    if !solved
        return zeros(length(parameter_values))
    end
    
    # Step 4: Initialize state
    state = [zeros(TT.nVars)]
    
    # Step 5: Run inversion filter with rrule
    llh, inversion_pullback = rrule(
        calculate_inversion_filter_loglikelihood, Val(:first_order),
        state, ğ’â‚, data_in_deviations, observables, TT;
        warmup_iterations = warmup_iterations,
        presample_periods = presample_periods,
        filter_algorithm = filter_algorithm,
        opts = opts
    )
    
    # Backward pass
    âˆ‚llh = 1.0
    
    # Inversion pullback: returns NoTangent, NoTangent, âˆ‚state, âˆ‚ğ’, âˆ‚data_in_deviations, NoTangent, ...
    pullback_result = inversion_pullback(âˆ‚llh)
    âˆ‚state = pullback_result[3]
    âˆ‚ğ’â‚ = pullback_result[4]
    âˆ‚data_in_deviations = pullback_result[5]
    
    # Gradient w.r.t. SS_and_pars from data_in_deviations
    âˆ‚SS_and_pars_data = zeros(length(SS_and_pars))
    âˆ‚SS_and_pars_data[obs_indices] = -sum(âˆ‚data_in_deviations, dims=2)[:]
    
    # First-order solution pullback: âˆ‚ğ’â‚ â†’ âˆ‚âˆ‡â‚
    _, âˆ‚âˆ‡â‚, _ = first_order_pullback((âˆ‚ğ’â‚, nothing, nothing))
    
    # Jacobian pullback: âˆ‚âˆ‡â‚ â†’ âˆ‚parameters, âˆ‚SS_and_pars
    _, âˆ‚parameters_jac, âˆ‚SS_and_pars_jac, _ = jacobian_pullback(âˆ‚âˆ‡â‚)
    
    # NSSS pullback: âˆ‚SS_and_pars â†’ âˆ‚parameters
    âˆ‚SS_and_pars_total = âˆ‚SS_and_pars_jac + âˆ‚SS_and_pars_data
    _, _, âˆ‚parameters_nsss, _ = nsss_pullback((âˆ‚SS_and_pars_total, nothing))
    
    # Total gradient w.r.t. parameters
    âˆ‚parameters = âˆ‚parameters_jac + âˆ‚parameters_nsss
    
    return âˆ‚parameters
end

function Mooncake.rrule!!(
    ::Mooncake.CoDual{typeof(get_loglikelihood)},
    ğ“‚_dual::Mooncake.CoDual{â„³},
    data_dual::Mooncake.CoDual{KeyedArray{Float64}},
    parameter_values_dual::Mooncake.CoDual{Vector{Float64}};
    algorithm::Symbol = :first_order,
    filter::Symbol = algorithm == :first_order ? :kalman : :inversion,
    on_failure_loglikelihood::Float64 = -Inf,
    warmup_iterations::Int = 0,
    presample_periods::Int = 0,
    initial_covariance::Symbol = :theoretical,
    filter_algorithm::Symbol = :LagrangeNewton,
    tol::Tolerances = Tolerances(),
    quadratic_matrix_equation_algorithm::Symbol = :schur,
    lyapunov_algorithm::Symbol = :doubling,
    sylvester_algorithm::Union{Symbol,Vector{Symbol},Tuple{Symbol,Vararg{Symbol}}} = :doubling,
    verbose::Bool = false
)
    # Extract primal values
    ğ“‚ = Mooncake.primal(ğ“‚_dual)
    data = Mooncake.primal(data_dual)
    parameter_values = Mooncake.primal(parameter_values_dual)
    
    # Get the tangent storage for parameter_values
    âˆ‚parameter_values = Mooncake.tangent(parameter_values_dual)
    
    # Setup calculation options
    opts = merge_calculation_options(tol = tol, verbose = verbose,
                    quadratic_matrix_equation_algorithm = quadratic_matrix_equation_algorithm,
                    sylvester_algorithmÂ² = isa(sylvester_algorithm, Symbol) ? sylvester_algorithm : sylvester_algorithm[1],
                    sylvester_algorithmÂ³ = (isa(sylvester_algorithm, Symbol) || length(sylvester_algorithm) < 2) ? 
                        (sum(k * (k + 1) Ã· 2 for k in 1:ğ“‚.timings.nPast_not_future_and_mixed + 1 + ğ“‚.timings.nExo) > DEFAULT_SYLVESTER_THRESHOLD ? 
                            DEFAULT_LARGE_SYLVESTER_ALGORITHM : DEFAULT_SYLVESTER_ALGORITHM) : 
                        sylvester_algorithm[2],
                    lyapunov_algorithm = lyapunov_algorithm)
    
    # Normalize options  
    filter_norm, _, algorithm_norm, _, _, warmup_iterations_norm = normalize_filtering_options(filter, false, algorithm, false, warmup_iterations)
    
    # Get observables
    observables = get_and_check_observables(ğ“‚, data)
    
    # Check bounds
    bounds_violated = check_bounds(parameter_values, ğ“‚)
    
    if bounds_violated
        # Return failure value and zero gradient
        function fail_pb!!(âˆ‚llh)
            return Mooncake.NoTangent(), Mooncake.NoTangent(), Mooncake.NoTangent(), âˆ‚parameter_values
        end
        return Mooncake.CoDual(on_failure_loglikelihood, zero_tangent(on_failure_loglikelihood)), fail_pb!!
    end
    
    # Compute forward pass
    llh = get_loglikelihood(ğ“‚, data, parameter_values;
                           algorithm = algorithm,
                           filter = filter,
                           on_failure_loglikelihood = on_failure_loglikelihood,
                           warmup_iterations = warmup_iterations,
                           presample_periods = presample_periods,
                           initial_covariance = initial_covariance,
                           filter_algorithm = filter_algorithm,
                           tol = tol,
                           quadratic_matrix_equation_algorithm = quadratic_matrix_equation_algorithm,
                           lyapunov_algorithm = lyapunov_algorithm,
                           sylvester_algorithm = sylvester_algorithm,
                           verbose = verbose)
    
    # Prepare data for gradient computation
    NSSS_labels = [sort(union(ğ“‚.exo_present, ğ“‚.var))..., ğ“‚.calibration_equations_parameters...]
    obs_indices = convert(Vector{Int}, indexin(observables, NSSS_labels))
    
    # Get steady state for data transformation
    SS_and_pars, _ = get_NSSS_and_parameters(ğ“‚, parameter_values; opts = opts)
    
    if collect(axiskeys(data,1)) isa Vector{String}
        data_rekey = rekey(data, 1 => axiskeys(data,1) .|> Meta.parse .|> replace_indices)
    else
        data_rekey = data
    end
    
    dt = collect(data_rekey(observables))
    data_in_deviations = dt .- SS_and_pars[obs_indices]
    
    # Compute analytical gradient based on algorithm and filter combination
    cached_grad = if algorithm_norm == :first_order && filter_norm == :kalman
        compute_analytical_gradient_first_order_kalman(
            ğ“‚, parameter_values, data_in_deviations, observables, obs_indices,
            presample_periods, initial_covariance, opts
        )
    elseif algorithm_norm == :first_order && filter_norm == :inversion
        compute_analytical_gradient_first_order_inversion(
            ğ“‚, parameter_values, data_in_deviations, observables, obs_indices,
            presample_periods, warmup_iterations_norm, filter_algorithm, opts
        )
    else
        # For higher-order algorithms (second_order, pruned_second_order, third_order, pruned_third_order)
        # Fall back to a simpler implementation or zero gradient with warning
        @warn "Mooncake analytical gradient not yet implemented for algorithm=$algorithm_norm, filter=$filter_norm. Returning zero gradient."
        zeros(length(parameter_values))
    end
    
    # Define pullback function
    function get_loglikelihood_pb!!(âˆ‚llh::Float64)
        # Accumulate gradient into âˆ‚parameter_values
        if âˆ‚parameter_values !== nothing
            âˆ‚parameter_values .+= cached_grad .* âˆ‚llh
        end
        # Return NoTangent for non-differentiable arguments
        return Mooncake.NoTangent(), Mooncake.NoTangent(), Mooncake.NoTangent(), âˆ‚parameter_values
    end
    
    # Return CoDual with primal and zero tangent, plus the pullback
    return Mooncake.CoDual(llh, zero_tangent(llh)), get_loglikelihood_pb!!
end

end # module
