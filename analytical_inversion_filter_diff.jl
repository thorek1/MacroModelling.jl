using Revise
using MacroModelling
using StatsPlots
using Zygote, ForwardDiff, FiniteDiff
using Random
import Optim, LineSearches
using BenchmarkTools
import LinearAlgebra as â„’

@model Gali_2015_chapter_3_nonlinear begin
	W_real[0] = C[0] ^ Ïƒ * N[0] ^ Ï†

	Q[0] = Î² * (C[1] / C[0]) ^ (-Ïƒ) * Z[1] / Z[0] / Pi[1]

	R[0] = 1 / Q[0]

	Y[0] = A[0] * (N[0] / S[0]) ^ (1 - Î±)

	R[0] = Pi[1] * realinterest[0]

	R[0] = 1 / Î² * Pi[0] ^ Ï•áµ–â± * (Y[0] / Y[ss]) ^ Ï•Ê¸ * exp(nu[0])

	C[0] = Y[0]

	log(A[0]) = Ï_a * log(A[-1]) + std_a * eps_a[x]

	log(Z[0]) = Ï_z * log(Z[-1]) - std_z * eps_z[x]

	nu[0] = Ï_Î½ * nu[-1] + std_nu * eps_nu[x]

	MC[0] = W_real[0] / (S[0] * Y[0] * (1 - Î±) / N[0])

	1 = Î¸ * Pi[0] ^ (Ïµ - 1) + (1 - Î¸) * Pi_star[0] ^ (1 - Ïµ)

	S[0] = (1 - Î¸) * Pi_star[0] ^ (( - Ïµ) / (1 - Î±)) + Î¸ * Pi[0] ^ (Ïµ / (1 - Î±)) * S[-1]

	Pi_star[0] ^ (1 + Ïµ * Î± / (1 - Î±)) = Ïµ * x_aux_1[0] / x_aux_2[0] * (1 - Ï„) / (Ïµ - 1)

	x_aux_1[0] = MC[0] * Y[0] * Z[0] * C[0] ^ (-Ïƒ) + Î² * Î¸ * Pi[1] ^ (Ïµ + Î± * Ïµ / (1 - Î±)) * x_aux_1[1]

	x_aux_2[0] = Y[0] * Z[0] * C[0] ^ (-Ïƒ) + Î² * Î¸ * Pi[1] ^ (Ïµ - 1) * x_aux_2[1]

	log_y[0] = log(Y[0])

	log_W_real[0] = log(W_real[0])

	log_N[0] = log(N[0])

	pi_ann[0] = 4 * log(Pi[0])

	i_ann[0] = 4 * log(R[0])

	r_real_ann[0] = 4 * log(realinterest[0])

	M_real[0] = Y[0] / R[0] ^ Î·

end


@parameters Gali_2015_chapter_3_nonlinear begin
	Ïƒ = 1

	Ï† = 5

	Ï•áµ–â± = 1.5
	
	Ï•Ê¸ = 0.125

	Î¸ = 0.75

	Ï_Î½ = 0.5

	Ï_z = 0.5

	Ï_a = 0.9

	Î² = 0.99

	Î· = 3.77

	Î± = 0.25

	Ïµ = 9

	Ï„ = 0

    std_a = .01

    std_z = .05

    std_nu = .0025

end



Random.seed!(1)
data = simulate(Gali_2015_chapter_3_nonlinear)([:pi_ann,:W_real], :, :simulate)

get_loglikelihood(Gali_2015_chapter_3_nonlinear, data, Gali_2015_chapter_3_nonlinear.parameter_values)

Zygote.gradient(x->get_loglikelihood(Gali_2015_chapter_3_nonlinear, data,x), Gali_2015_chapter_3_nonlinear.parameter_values)[1]
ForwardDiff.gradient(x->get_loglikelihood(Gali_2015_chapter_3_nonlinear, data,x), Gali_2015_chapter_3_nonlinear.parameter_values)



ğ“‚ = Gali_2015_chapter_3_nonlinear

import MacroModelling: get_and_check_observables, check_bounds, minimize_distance_to_initial_data, get_relevant_steady_state_and_state_update, replace_indices, minimize_distance_to_data, match_data_sequence!, match_initial_data!

parameter_values = ğ“‚.parameter_values
algorithm = :first_order
filter = :inversion
warmup_iterations = 0
presample_periods = 0
initial_covariance = :diagonal
tol = 1e-12
verbose = false

observables = get_and_check_observables(ğ“‚, data)

solve!(ğ“‚, verbose = verbose, algorithm = algorithm)

bounds_violated = check_bounds(parameter_values, ğ“‚)

NSSS_labels = [sort(union(ğ“‚.exo_present, ğ“‚.var))..., ğ“‚.calibration_equations_parameters...]

obs_indices = convert(Vector{Int}, indexin(observables, NSSS_labels))

TT, SS_and_pars, ğ’, state, solved = get_relevant_steady_state_and_state_update(Val(algorithm), parameter_values, ğ“‚, tol)

if collect(axiskeys(data,1)) isa Vector{String}
    data = rekey(data, 1 => axiskeys(data,1) .|> Meta.parse .|> replace_indices)
end

dt = collect(data(observables))

# prepare data
data_in_deviations = dt .- SS_and_pars[obs_indices]



T = ğ“‚.timings

precision_factor = 1.0

n_obs = size(data_in_deviations,2)

cond_var_idx = indexin(observables,sort(union(T.aux,T.var,T.exo_present)))





state = [zeros(T.nVars) for _ in 1:size(data_in_deviations,2)+1]
# statetmp = zeros(23)
shocksÂ² = 0.0
logabsdets = 0.0
y = zeros(length(cond_var_idx))
x = [zeros(T.nExo) for _ in 1:size(data_in_deviations,2)]
# state_reduced = zeros(T.nPast_not_future_and_mixed + T.nExo)

jac = ğ’[cond_var_idx,end-T.nExo+1:end]

if T.nExo == length(observables)
    logabsdets = â„’.logabsdet(-jac' ./ precision_factor)[1]
    jacdecomp = â„’.lu(jac)
    invjac = inv(jacdecomp)
else
    logabsdets = sum(x -> log(abs(x)), â„’.svdvals(-jac' ./ precision_factor))
    jacdecomp = â„’.svd(jac)
    invjac = inv(jacdecomp)
end

logabsdets *= size(data_in_deviations,2) - presample_periods

@views ğ’obs = ğ’[cond_var_idx,1:end-T.nExo]

for i in axes(data_in_deviations,2)
    @views â„’.mul!(y, ğ’obs, state[i][T.past_not_future_and_mixed_idx])
    @views â„’.axpby!(1, data_in_deviations[:,i], -1, y)
    â„’.mul!(x[i],invjac,y)
    # x = invjac * (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])
    # x = ğ’[cond_var_idx,end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])

    if i > presample_periods
        shocksÂ² += sum(abs2,x[i])
    end

    # # copyto!(state_reduced, 1, state, T.past_not_future_and_mixed_idx)
    # for (i,v) in enumerate(T.past_not_future_and_mixed_idx)
    #     state_reduced[i] = state[v]
    # end
    # copyto!(state_reduced, T.nPast_not_future_and_mixed + 1, x, 1, T.nExo)
    
    â„’.mul!(state[i+1], ğ’, vcat(state[i][T.past_not_future_and_mixed_idx], x[i]))
    # state[i+1] =  ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], x[i])
    # state = state_update(state, x)
end

-(logabsdets + shocksÂ² + (length(observables) * (warmup_iterations + n_obs - presample_periods)) * log(2 * 3.141592653589793)) / 2



# for i in axes(data_in_deviations,2)
#     x = ğ’[cond_var_idx,end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])
#     state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], x[i])
#     shocksÂ² += sum(abs2,x[i])
# end
# return shocksÂ²
ğ’endo = ğ’[cond_var_idx, 1:end-T.nExo]
ğ’exo = ğ’[cond_var_idx, end-T.nExo+1:end]


# âˆ‚state = zero(state[1])
âˆ‚x = zero(x[1])
âˆ‚ğ’ = zero(ğ’)
âˆ‚v = zero(data_in_deviations[:,1])
âˆ‚data_in_deviationsâˆ‚x = zero(data_in_deviations)


for i in 2:-1:1 # reverse(axes(data_in_deviations,2))
    # âˆ‚âˆ‚data_in_deviationsâˆ‚shockÂ²
    âˆ‚x = 2*x[i]
 
    âˆ‚v = invjac' * âˆ‚x

    âˆ‚data_in_deviationsâˆ‚x[:,i] = âˆ‚v

    if i < size(data_in_deviations,2)
        âˆ‚data_in_deviationsâˆ‚x[:,i] -= invjac' * (ğ’[T.past_not_future_and_mixed_idx,:]' * ğ’endo' * invjac' * 2 * x[i+1])[end-T.nExo+1:end]
    end


    âˆ‚ğ’[cond_var_idx, end-T.nExo+1:end] -= invjac' * 2 * x[i] * x[i]' # [cond_var_idx, end-T.nExo+1:end]

    # âˆ‚ğ’[cond_var_idx, end-T.nExo+1:end] -= invjac' * 2 * x[i+1] * x[i+1]' # [cond_var_idx, end-T.nExo+1:end]

    if i > 1
        âˆ‚ğ’[cond_var_idx, 1:end-T.nExo] -= (ğ’[st,:] * vcat(state[i-1][st], x[i-1]))' .* (invjac' * 2 * x[i])
        âˆ‚ğ’[cond_var_idx, 1:end-T.nExo] -= invjac' * (ğ’[st,:]' * ğ’[cond_var_idx, 1:end-T.nExo]' * invjac' * 2 * x[i])[end-T.nExo+1:end] * state[i-1][st]'

        âˆ‚ğ’[st,:] -= vcat(state[i-1][st], x[i-1])' .* (ğ’[cond_var_idx, 1:end-T.nExo]' * invjac' * 2 * x[i])

        âˆ‚ğ’[cond_var_idx, end-T.nExo+1:end] += invjac' * (ğ’[st,:]' * ğ’[cond_var_idx, 1:end-T.nExo]' * invjac' * 2 * x[i])[end-T.nExo+1:end] * x[i-1]'

    end
    # âˆ‚ğ’âˆ‚shockÂ²
    # v = (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])
    
    # âˆ‚ğ’[cond_var_idx,end-T.nExo+1:end] -= âˆ‚v * x[i]'# - (v - jac * x[i]) * âˆ‚v' * invjac' - invjac' * x[i] * (âˆ‚x' - âˆ‚v' * jac)

    ### state = ğ’ * vcat(state[T.past_not_future_and_mixed_idx], x)

    # if i < size(data_in_deviations,2)
    #     âˆ‚ğ’ += âˆ‚state * vcat(state[i][T.past_not_future_and_mixed_idx], x[i+1])'
    # end

    # âˆ‚state[T.past_not_future_and_mixed_idx] += ğ’[:,1:end-T.nExo]' * âˆ‚state

    # âˆ‚x += ğ’[:,end-T.nExo+1:end]' * âˆ‚state

    ### x = âˆ‚ğ’[cond_var_idx,end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx]))


    # âˆ‚ğ’[cond_var_idx,1:end-T.nExo] -= invjac' * âˆ‚x * state[i][T.past_not_future_and_mixed_idx]'

    # âˆ‚state[T.past_not_future_and_mixed_idx] -= ğ’[cond_var_idx,1:end-T.nExo]' * invjac' * âˆ‚x
end



res = FiniteDiff.finite_difference_gradient(ğ’ -> begin
# ForwardDiff.gradient(x->begin
# Zygote.gradient(x->begin
    shocksÂ² = 0.0
    for i in 1:2 # axes(data_in_deviations,2)
        X = ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])

        state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], X)

        shocksÂ² += sum(abs2,X)
    end

    return shocksÂ²
end, ğ’)#_in_deviations[:,1:2])

isapprox(res, âˆ‚ğ’, rtol = eps(Float32))

res - âˆ‚ğ’


FiniteDiff.finite_difference_gradient(ğ’exo -> sum(abs2, ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])), ğ’exo)# + âˆ‚v


# there are multiple parts to it. first the effect of the previous iteration through this one and then the direct effect

uuuu = (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])

uuu = vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ uuuu)

uu = ğ’[st,:] * uuu

u = data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * uu

X = ğ’[cond_var_idx, end-T.nExo+1:end] \ u

sum(abs2, X)


sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st]))))


Zygote.jacobian(x -> vcat(state[i][st], x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])), ğ’[cond_var_idx, end-T.nExo+1:end])[1]

Zygote.jacobian(x ->  x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st]), ğ’[cond_var_idx, end-T.nExo+1:end])[1]


Zygote.gradient(x -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * ğ’[st,:] * vcat(state[i][st], x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))), ğ’[cond_var_idx, end-T.nExo+1:end])[1]

Zygote.gradient(x -> sum(abs2, x \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * ğ’[st,:] * vcat(state[i][st], x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))), ğ’[cond_var_idx, end-T.nExo+1:end])[1]


Zygote.gradient(x -> sum(abs2,  x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])), ğ’[cond_var_idx, end-T.nExo+1:end])[1] + Zygote.gradient(x -> sum(abs2, x \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] *  ğ’[st,:] * vcat(state[i][st], x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))), ğ’[cond_var_idx, end-T.nExo+1:end])[1]



Zygote.gradient(x -> sum(abs2,  x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])), ğ’[cond_var_idx, end-T.nExo+1:end])[1]

Zygote.gradient(x -> sum(abs2, x \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] *  ğ’[st,:] * vcat(state[i][st], x \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))), ğ’[cond_var_idx, end-T.nExo+1:end])[1]



âˆ‚ğ’ = zero(ğ’)

i = 1
âˆ‚xâˆ‚shocksÂ² = 2 * x[i]

âˆ‚uâˆ‚x = invjac'

âˆ‚ğ’âˆ‚shocksÂ² = âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ² * x[i]' # [cond_var_idx, end-T.nExo+1:end]

âˆ‚ğ’[cond_var_idx, end-T.nExo+1:end] -= invjac' * 2 * x[i] * x[i]' # [cond_var_idx, end-T.nExo+1:end]

âˆ‚ğ’[cond_var_idx, end-T.nExo+1:end] -= invjac' * 2 * x[i+1] * x[i+1]' # [cond_var_idx, end-T.nExo+1:end]


# next S
âˆ‚xâˆ‚shocksÂ² = 2 * x[i+1]

âˆ‚ğ’âˆ‚u = - (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))'

âˆ‚ğ’2âˆ‚shocksÂ² = âˆ‚ğ’âˆ‚u .* (âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ²)

âˆ‚ğ’2âˆ‚shocksÂ² = - (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))' .* (âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ²)

âˆ‚ğ’[cond_var_idx, 1:end-T.nExo] -= (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))' .* (invjac' * 2 * x[i+1])


# next S
âˆ‚uuâˆ‚u = - ğ’[cond_var_idx, 1:end-T.nExo]'

âˆ‚ğ’âˆ‚uu = uuu'

âˆ‚ğ’2âˆ‚shocksÂ² = âˆ‚ğ’âˆ‚uu .* (âˆ‚uuâˆ‚u * âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ²)

âˆ‚ğ’[st,:] += (vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))' .* (- ğ’[cond_var_idx, 1:end-T.nExo]' * invjac' * 2 * x[i+1])


# next S
i = 1
âˆ‚xâˆ‚shocksÂ² = 2 * x[i+1]

âˆ‚uâˆ‚x = invjac'

âˆ‚uuâˆ‚u = ğ’[cond_var_idx, 1:end-T.nExo]'

âˆ‚uuuâˆ‚uu = ğ’[st,:]'

âˆ‚ğ’âˆ‚shocksÂ² = invjac' * (âˆ‚uuuâˆ‚uu * âˆ‚uuâˆ‚u * âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ²)[end-T.nExo+1:end] * x[i]'

âˆ‚ğ’[cond_var_idx, end-T.nExo+1:end] += invjac' * (ğ’[st,:]' * ğ’[cond_var_idx, 1:end-T.nExo]' * invjac' * 2 * x[i+1])[end-T.nExo+1:end] * x[i]'



# next S
i = 1
âˆ‚xâˆ‚shocksÂ² = 2 * x[i+1]

âˆ‚uâˆ‚x = invjac'

âˆ‚uuâˆ‚u = ğ’[cond_var_idx, 1:end-T.nExo]'

âˆ‚uuuâˆ‚uu = ğ’[st,:]'

âˆ‚uuuuâˆ‚uuu = invjac'

âˆ‚ğ’âˆ‚uuuu = - state[i][st]'

âˆ‚uuuuâˆ‚uuu * (âˆ‚uuuâˆ‚uu * âˆ‚uuâˆ‚u * âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ²)[end-T.nExo+1:end] * âˆ‚ğ’âˆ‚uuuu

# âˆ‚ğ’[cond_var_idx, 1:end-T.nExo] += âˆ‚uuuuâˆ‚uuu * (âˆ‚uuuâˆ‚uu * âˆ‚uuâˆ‚u * âˆ‚uâˆ‚x * âˆ‚xâˆ‚shocksÂ²)[end-T.nExo+1:end] * âˆ‚ğ’âˆ‚uuuu

âˆ‚ğ’[cond_var_idx, 1:end-T.nExo] -= invjac' * (ğ’[st,:]' * ğ’[cond_var_idx, 1:end-T.nExo]' * invjac' * 2 * x[i+1])[end-T.nExo+1:end] * state[i][st]'




# u = 

ForwardDiff.gradient(XX -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] -XX * uu)), ğ’[cond_var_idx, 1:end-T.nExo][:,:])


ForwardDiff.jacobian(XX -> -XX * uu, ğ’[cond_var_idx, 1:end-T.nExo][:,:])

ForwardDiff.gradient(XX -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] -XX * uu)), ğ’[cond_var_idx, 1:end-T.nExo][:,:])

Zygote.gradient(XX -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] -XX * uu)), ğ’[cond_var_idx, 1:end-T.nExo][:,:])[1]


Zygote.gradient(XX -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ XX), (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * uu))[1]


âˆ‚data_in_deviationsâˆ‚x[:,i] -= invjac' * (ğ’[T.past_not_future_and_mixed_idx,:]' * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])) * ğ’endo' * invjac' * 2 * x[i+1])[end-T.nExo+1:end]



FiniteDiff.finite_difference_jacobian(x -> data_in_deviations[:,i+1] - x * (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st]))), ğ’[cond_var_idx, 1:end-T.nExo])



ForwardDiff.gradient(XX -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] - XX * (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st]))))), ğ’[cond_var_idx, 1:end-T.nExo])

ForwardDiff.jacobian(x -> data_in_deviations[:,i+1] - x * (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st]))), ğ’[cond_var_idx, 1:end-T.nExo])

xxx = ForwardDiff.jacobian(x -> - x * (ğ’[st,:] * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st]))), ğ’[cond_var_idx, 1:end-T.nExo])


sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * (ğ’ * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))[st]))


# starting with the iterated indirect effect



FiniteDiff.finite_difference_gradient(ğ’ -> sum(abs2, ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i+1] - ğ’[cond_var_idx, 1:end-T.nExo] * (ğ’ * vcat(state[i][st], ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][st])))[st])), ğ’)# + âˆ‚v

FiniteDiff.finite_difference_gradient(ğ’exo2 -> sum(abs2, ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], ğ’exo2 \ (data_in_deviations[:,i] - ğ’endo * state[i][st])))[st])), ğ’exo)# + âˆ‚v

invjac' * (ğ’[T.past_not_future_and_mixed_idx,:]' * ğ’endo'  * invjac' * âˆ‚ğ’[cond_var_idx,end-T.nExo+1:end]')[end-T.nExo+1:end,:]

res = FiniteDiff.finite_difference_gradient(ğ’ -> begin
# ForwardDiff.gradient(x->begin
# Zygote.gradient(x->begin
    shocksÂ² = 0.0
    for i in 1:2 # axes(data_in_deviations,2)
        X = ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])

        state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], X)

        shocksÂ² += sum(abs2,X)
    end

    return shocksÂ²
end, ğ’)#_in_deviations[:,1:2])

isapprox(res, âˆ‚ğ’, rtol = eps(Float32))

res - âˆ‚ğ’

FiniteDiff.finite_difference_gradient(data_in_deviations -> begin
# ForwardDiff.gradient(x->begin
# Zygote.gradient(x->begin
    shocksÂ² = 0.0
    for i in 1:10 # axes(data_in_deviations,2)
        X = ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][T.past_not_future_and_mixed_idx])

        state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], X)

        shocksÂ² += sum(abs2,X)
    end

    return shocksÂ²
end, data_in_deviations[:,1:10])


âˆ‚state *= 0

âˆ‚x = 2*x[1]

âˆ‚state[T.past_not_future_and_mixed_idx] += ğ’[:,1:end-T.nExo]' * âˆ‚state

âˆ‚x += ğ’[:,end-T.nExo+1:end]' * âˆ‚state

âˆ‚v = invjac' * âˆ‚x

âˆ‚data_in_deviationsâˆ‚x[:,1] = âˆ‚v


âˆ‚data_in_deviations = zero(data_in_deviations)

âˆ‚shocksÂ² = 2*x[1]

# x = ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])



# x = sum(abs2, ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])))[st]))

i = 1
st = T.past_not_future_and_mixed_idx

âˆ‚xâˆ‚shocksÂ² = 2 * x[1]

âˆ‚vâˆ‚x = invjac'

âˆ‚vâˆ‚shocksÂ² = âˆ‚vâˆ‚x * âˆ‚xâˆ‚shocksÂ²

vsub = ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])

vvv = vcat(state[i][st], vsub)

vvv = vcat(state[i][st], ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st]))

vv =  (ğ’[st,:] * vvv)

v = (data_in_deviations[:,i+1] - ğ’endo * vv)

âˆ‚vvâˆ‚v = - ğ’endo'

âˆ‚vvâˆ‚shocksÂ² = âˆ‚vvâˆ‚v * âˆ‚vâˆ‚x * âˆ‚xâˆ‚shocksÂ²

âˆ‚vvvâˆ‚vv = ğ’[st,:]'

âˆ‚vvvâˆ‚shocksÂ² = âˆ‚vvvâˆ‚vv * âˆ‚vvâˆ‚v * âˆ‚vâˆ‚x * âˆ‚xâˆ‚shocksÂ²

âˆ‚vsubuâˆ‚vvv = â„’.I(size(ğ’,2))[:,end-T.nExo+1:end]'

âˆ‚vsubâˆ‚shocksÂ² = âˆ‚vsubuâˆ‚vvv * âˆ‚vvvâˆ‚vv * âˆ‚vvâˆ‚v * âˆ‚vâˆ‚x * âˆ‚xâˆ‚shocksÂ²
âˆ‚vsubâˆ‚shocksÂ² = (âˆ‚vvvâˆ‚vv * âˆ‚vvâˆ‚v * âˆ‚vâˆ‚x * âˆ‚xâˆ‚shocksÂ²)[end-T.nExo+1:end]

âˆ‚datâˆ‚shocksÂ² = invjac' * (âˆ‚vvvâˆ‚vv * âˆ‚vvâˆ‚v * âˆ‚vâˆ‚x * âˆ‚xâˆ‚shocksÂ²)[end-T.nExo+1:end]

âˆ‚datâˆ‚shocksÂ² = -invjac' * (ğ’[st,:]' * ğ’endo' * invjac' * 2 * x[2])[end-T.nExo+1:end]

âˆ‚datâˆ‚shocksÂ² = -ğ’exo' \ (ğ’[st,:]' * ğ’endo' / ğ’exo' * 2 * x[2])[end-T.nExo+1:end]

âˆ‚datâˆ‚shocksÂ² = -ğ’exo' \ (2 * x[1])


# âˆ‚xâˆ‚v = 

# shocksÂ² = sum(abs2, ğ’exo \ v)

invjac' * ğ’[st,1:end-T.nExo] * ğ’endo' * invjac' * 2 * x[1]

âˆ‚shocksÂ² = 2 * (ğ’exo \ v)' * âˆ‚shocksÂ²
âˆ‚v = (âˆ‚shocksÂ² / shocksÂ²) * (ğ’exo \ v)'

# x = sum(abs2, ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])))[st]))
# i = 1

FiniteDiff.finite_difference_gradient(x -> sum(abs2, ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], ğ’exo \ (x - ğ’endo * state[i][st])))[st])), data_in_deviations[:,i])# + âˆ‚v

FiniteDiff.finite_difference_gradient(x -> sum(abs2, ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * ğ’[st,:] * vcat(state[i][st], ğ’exo \ (x - ğ’endo * state[i][st])))), data_in_deviations[:,i])# + âˆ‚v


2 * (ğ’exo \ (data_in_deviations[:, 2] - ğ’endo * (ğ’ * vcat(state[1][st], ğ’exo \ (data_in_deviations[:, 1] - ğ’endo * state[1][st])))[st]))

2 * (ğ’exo \ (data_in_deviations[:, 2] - ğ’endo * (ğ’ * vcat(state[1][st], ğ’exo \ (data_in_deviations[:, 1] - ğ’endo * state[1][st])))[st])) * (-invjac * ğ’endo[:, st] * ğ’[cond_var_idx,:]' * invjac')

-2 * x[1] * ğ’endo * invjac * ğ’
X = ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], x))[st])

X = ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * state[i+1][st])

âˆ‚data_in_deviations[:,1] = invjac' * âˆ‚shocksÂ²

# âˆ‚state[end-T.nExo+1:end] = 

FiniteDiff.finite_difference_gradient(data_in_deviations -> begin
# ForwardDiff.gradient(x->begin
# Zygote.gradient(x->begin
    shocksÂ² = 0.0
    for i in 1:10 # axes(data_in_deviations,2)
        X = ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][T.past_not_future_and_mixed_idx])

        state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], X)

        shocksÂ² += sum(abs2,X)
    end

    return shocksÂ²
end, data_in_deviations[:,1:10])


data = copy(data_in_deviations);
data[:,2:3] *= 0;

FiniteDiff.finite_difference_gradient(data_in_deviations -> begin
# ForwardDiff.gradient(x->begin
# Zygote.gradient(x->begin
    shocksÂ² = 0.0
    for i in axes(data_in_deviations,2)
        X = ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])

        state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], X)

        shocksÂ² += sum(abs2,X)
    end

    return shocksÂ²
end, data)#_in_deviations[:,1:2])



i = 1


st = T.past_not_future_and_mixed_idx

# data has an impact because of the difference between today and tomorrow, as in the data matters for two periods
# fisrt period:
X = ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])

# second period:
state[i+1] = ğ’ * vcat(state[i][st], X)

# here it matters because it is part of X -> state and thereby pushes around the deterministic part of the system (via the state)
# X = ğ’[cond_var_idx, end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx, 1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])
X = ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], ğ’exo \ (data_in_deviations[:,i] - ğ’endo * state[i][st])))[st])

X = ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * (ğ’ * vcat(state[i][st], X))[st])

X = ğ’exo \ (data_in_deviations[:,i+1] - ğ’endo * state[i+1][st])

1

# state::Vector{Vector{Float64}}, 
#                                                     ğ’::Union{Matrix{Float64}, Vector{AbstractMatrix{Float64}}}, 
#                                                     data_in_deviations::Matrix{Float64}, 
#                                                     observables::Union{Vector{String}, Vector{Symbol}},
#                                                     T::timings; 
#                                                     warmup_iterations::Int = 0,
#                                                     presample_periods::Int = 0)


# function first_order_state_update(state::Vector{U}, shock::Vector{S}) where {U <: Real,S <: Real}
# # state_update = function(state::Vector{T}, shock::Vector{S}) where {T <: Real,S <: Real}
#     aug_state = [state[T.past_not_future_and_mixed_idx]
#                 shock]
#     return ğ’ * aug_state # you need a return statement for forwarddiff to work
# end

# state_update = first_order_state_update

# state = state[1]

# pruning = false

    
# precision_factor = 1.0

# n_obs = size(data_in_deviations,2)

# cond_var_idx = indexin(observables,sort(union(T.aux,T.var,T.exo_present)))



# warmup_iterations = 3
# state *=0

# data_in_deviations[:,1] - (ğ’ * vcat((ğ’ * vcat(state[T.past_not_future_and_mixed_idx], (x[1:3])))[T.past_not_future_and_mixed_idx], zero(x[1:3])) + ğ’ * vcat(state[T.past_not_future_and_mixed_idx], x[4:6]))[cond_var_idx]

# data_in_deviations[:,1] - (ğ’ * (vcat((ğ’[T.past_not_future_and_mixed_idx,:] * vcat(state[T.past_not_future_and_mixed_idx], (x[1:3]))), zero(x[1:3])) + vcat(state[T.past_not_future_and_mixed_idx], x[4:6])))[cond_var_idx]


# data_in_deviations[:,1] - (ğ’[cond_var_idx,:] * (vcat((ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end] * x[1:3]), zero(x[1:3])) + vcat(state[T.past_not_future_and_mixed_idx], x[4:6])))


# ğ’[cond_var_idx,:] * (vcat((ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end] * x[1:3]), zero(x[1:3])))

# ğ’[cond_var_idx,:] * vcat(state[T.past_not_future_and_mixed_idx], x[4:6])


# ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * (ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end] * x[1:3]) + ğ’[cond_var_idx,end-T.nExo+1:end] * x[4:6] - data_in_deviations[:,1]




# ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end] * x[1:3] +
# ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end] * x[4:6] +
# ğ’[cond_var_idx,end-T.nExo+1:end] * x[7:9] -
# data_in_deviations[:,1]

# hcat(   
#     ğ’[cond_var_idx,end-T.nExo+1:end] , 
#     ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end], 
#     ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end]
#     ) \ data_in_deviations[:,1]


# warmup_iterations = 5

# state *= 0
# logabsdets = 0
# shocksÂ² = 0

# if warmup_iterations > 0
#     if warmup_iterations >= 1
#         to_be_inverted = ğ’[cond_var_idx,end-T.nExo+1:end]
#         if warmup_iterations >= 2
#             to_be_inverted = hcat(ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end], to_be_inverted)
#             if warmup_iterations >= 3
#                 Sáµ‰ = ğ’[T.past_not_future_and_mixed_idx,1:T.nPast_not_future_and_mixed]
#                 for e in 1:warmup_iterations-2
#                     to_be_inverted = hcat(ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * Sáµ‰ * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end], to_be_inverted)
#                     Sáµ‰ *= ğ’[T.past_not_future_and_mixed_idx,1:T.nPast_not_future_and_mixed]
#                 end
#             end
#         end
#     end

#     x = to_be_inverted \ data_in_deviations[:,1]

#     warmup_shocks = reshape(x, T.nExo, warmup_iterations)

#     for i in 1:warmup_iterations-1
#         state = state_update(state, warmup_shocks[:,i])
#     end

#     jacc = -to_be_inverted'

#     for i in 1:warmup_iterations
#         if T.nExo == length(observables)
#             logabsdets += â„’.logabsdet(jacc[(i - 1) * T.nExo .+ (1:2),:] ./ precision_factor)[1]
#         else
#             logabsdets += sum(x -> log(abs(x)), â„’.svdvals(jacc[(i - 1) * T.nExo .+ (1:2),:] ./ precision_factor))
#         end
#     end

#     shocksÂ² += sum(abs2,x)
# end



# data_in_deviations[:,1] - ğ’[cond_var_idx,:] * vcat((ğ’ * vcat(state[T.past_not_future_and_mixed_idx], (x[1:3])))[T.past_not_future_and_mixed_idx], zero(x[1:3])) + ğ’[cond_var_idx,:] * vcat(state[T.past_not_future_and_mixed_idx], x[4:6])



# data_in_deviations[:,1]

# (ğ’ * vcat(state[T.past_not_future_and_mixed_idx], x[1:3]))[cond_var_idx]


#         x = invjac * (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])

#         â„’.mul!(state, ğ’, vcat(state[T.past_not_future_and_mixed_idx], x))





# state_copy = deepcopy(state)

# XX = reshape(X, length(X) Ã· warmup_iters, warmup_iters)

# for i in 1:warmup_iters
#     state_copy = state_update(state_copy, XX[:,i])
# end

# return precision_factor .* sum(abs2, data - (pruning ? sum(state_copy) : state_copy)[cond_var_idx])



# shocksÂ² = 0.0
# logabsdets = 0.0

# if warmup_iterations > 0
#     res = Optim.optimize(x -> minimize_distance_to_initial_data(x, data_in_deviations[:,1], state, state_update, warmup_iterations, cond_var_idx, precision_factor, pruning), 
#                         zeros(T.nExo * warmup_iterations), 
#                         Optim.LBFGS(linesearch = LineSearches.BackTracking(order = 3)), 
#                         Optim.Options(f_abstol = eps(), g_tol= 1e-30); 
#                         autodiff = :forward)

#     matched = Optim.minimum(res) < 1e-12

#     if !matched # for robustness try other linesearch
#         res = Optim.optimize(x -> minimize_distance_to_initial_data(x, data_in_deviations[:,1], state, state_update, warmup_iterations, cond_var_idx, precision_factor, pruning), 
#                         zeros(T.nExo * warmup_iterations), 
#                         Optim.LBFGS(), 
#                         Optim.Options(f_abstol = eps(), g_tol= 1e-30); 
#                         autodiff = :forward)
    
#         matched = Optim.minimum(res) < 1e-12
#     end

#     if !matched return -Inf end

#     x = Optim.minimizer(res)

#     warmup_shocks = reshape(x, T.nExo, warmup_iterations)

#     for i in 1:warmup_iterations-1
#         state = state_update(state, warmup_shocks[:,i])
#     end
    
#     res = zeros(0)

#     jacc = zeros(T.nExo * warmup_iterations, length(observables))

#     match_initial_data!(res, x, jacc, data_in_deviations[:,1], state, state_update, warmup_iterations, cond_var_idx, precision_factor), zeros(size(data_in_deviations, 1))

#     for i in 1:warmup_iterations
#         if T.nExo == length(observables)
#             logabsdets += â„’.logabsdet(jacc[(i - 1) * T.nExo .+ (1:2),:] ./ precision_factor)[1]
#         else
#             logabsdets += sum(x -> log(abs(x)), â„’.svdvals(jacc[(i - 1) * T.nExo .+ (1:2),:] ./ precision_factor))
#         end
#     end

#     shocksÂ² += sum(abs2,x)
# end



# jac = ğ’[cond_var_idx,end-T.nExo+1:end]

# jacdecomp = â„’.svd(jac)
# invjac = inv(jacdecomp)

# using FiniteDiff
# FiniteDiff.finite_difference_jacobian(xx -> sum(x -> log(abs(x)), â„’.svdvals(xx)),ğ’[cond_var_idx,end-T.nExo+1:end])

# ForwardDiff.jacobian(xx -> sum(x -> log(abs(x)), â„’.svdvals(xx)),ğ’[cond_var_idx,end-T.nExo+1:end])


# ForwardDiff.gradient(x-> x'*x,[1,2,3])
# [1,2,3]'*[1,2,3]


# âˆ‚det = -inv(â„’.svd(ğ’[cond_var_idx,end-T.nExo+1:end]))






state = zeros(T.nVars)
# statetmp = zeros(23)
shocksÂ² = 0.0
logabsdets = 0.0
y = zeros(length(cond_var_idx))
x = zeros(T.nExo)
# state_reduced = zeros(T.nPast_not_future_and_mixed + T.nExo)

jac = ğ’[cond_var_idx,end-T.nExo+1:end]

if T.nExo == length(observables)
    logabsdets = â„’.logabsdet(-jac' ./ precision_factor)[1]
    jacdecomp = â„’.lu!(jac)
    invjac = inv(jacdecomp)
else
    logabsdets = sum(x -> log(abs(x)), â„’.svdvals(-jac' ./ precision_factor))
    jacdecomp = â„’.svd!(jac)
    invjac = inv(jacdecomp)
end

logabsdets *= size(data_in_deviations,2) - presample_periods

@views ğ’obs = ğ’[cond_var_idx,1:end-T.nExo]

for i in axes(data_in_deviations,2)
    x = invjac * (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])

    if i > presample_periods
        shocksÂ² += sum(abs2,x)
    end

    state = ğ’ * vcat(state[T.past_not_future_and_mixed_idx], x)
end
shocksÂ²

-(logabsdets + shocksÂ² + (length(observables) * (warmup_iterations + n_obs - presample_periods)) * log(2 * 3.141592653589793)) / 2




inv(â„’.svd(ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end]))

inv(â„’.svd(ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed]))' * inv( â„’.svd( ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end]) )'

ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed] * inv( â„’.svd( ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end]) )'

inv(â„’.svd(ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed]))' *  ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end]


inv(â„’.svd(ğ’[T.past_not_future_and_mixed_idx,end-T.nExo+1:end])) * inv(â„’.svd(ğ’[cond_var_idx,1:T.nPast_not_future_and_mixed])) 

FiniteDiff.finite_difference_gradient(x->begin
# ForwardDiff.gradient(x->begin
# Zygote.gradient(x->begin
    shocksÂ² = 0.0
    X = zeros(eltype(x),T.nExo)

    for i in 1:2#xes(data_in_deviations,2)
        X = ğ’[cond_var_idx,end-T.nExo+1:end] \ (x[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])

        if i > presample_periods
            shocksÂ² += sum(abs2,X)
        end

        state[i+1] = ğ’ * vcat(state[i][T.past_not_future_and_mixed_idx], X)
    end
    return shocksÂ²
end, data_in_deviations[:,1:2])



ForwardDiff.gradient(x->sum(abs2, x[cond_var_idx,end-T.nExo+1:end] \ (data_in_deviations[:,i] - x[cond_var_idx,1:end-T.nExo] * state[1][T.past_not_future_and_mixed_idx])), ğ’)

ForwardDiff.gradient(x->sum(abs2, ğ’[cond_var_idx,end-T.nExo+1:end] \ (x - ğ’[cond_var_idx,1:end-T.nExo] * state[i][T.past_not_future_and_mixed_idx])), data_in_deviations[:,i])

ForwardDiff.gradient(x->sum(abs2, ğ’[cond_var_idx,end-T.nExo+1:end] \ (data_in_deviations[:,i] - x * state[i][T.past_not_future_and_mixed_idx])), ğ’[cond_var_idx,1:end-T.nExo])

ForwardDiff.gradient(x->sum(abs2, ğ’[cond_var_idx,end-T.nExo+1:end] \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * x)), state[i][T.past_not_future_and_mixed_idx])





(jac' * jac)' \ jac'

invjac' * jac' * invjac'
# âˆ‚jac =  (jac)' * (jac *  âˆ‚x)

FiniteDiff.finite_difference_jacobian(x->sum(abs2,x \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])),jac)

ForwardDiff.gradient(x->sum(abs2, x \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])),jac)

@profview for i in 1:10000 Zygote.gradient(x->sum(abs2, x \ (data_in_deviations[:,1] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])),jac) end

vec(jac) * vec(jac)'
ForwardDiff.gradient(x->â„’.det(inv(x)),jac[:,1:2])

(jac[:,1:2]) * (jac[:,1:2])'


vec(jac[:,1:2]) * vec(jac[:,1:2])'

âˆ‚data_in_deviationsâˆ‚x = invjac' * âˆ‚x

âˆ‚ğ’[cond_var_idx,1:end-T.nExo] = -invjac' * âˆ‚x * state[T.past_not_future_and_mixed_idx]'

âˆ‚state[T.past_not_future_and_mixed_idx] = -ğ’[cond_var_idx,1:end-T.nExo]' * invjac' * âˆ‚x

# state = ğ’ * vcat(state[T.past_not_future_and_mixed_idx], x)
âˆ‚ğ’ += âˆ‚state * vcat(state[T.past_not_future_and_mixed_idx], x)'

âˆ‚state[T.past_not_future_and_mixed_idx] += ğ’[:,1:end-T.nExo]' * âˆ‚state





if i > presample_periods
    shocksÂ² += sum(abs2,x)
end

state = ğ’ * vcat(state[T.past_not_future_and_mixed_idx], x)



ForwardDiff.gradient(x -> sum(abs2, x \ (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])), jac)

ForwardDiff.gradient(x -> sum(abs2, x * (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])), invjac)

ForwardDiff.gradient(x -> sum(abs2, invjac * (x - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])), data_in_deviations[:,i])

ForwardDiff.gradient(x -> sum(abs2, invjac * (data_in_deviations[:,i] - x * state[T.past_not_future_and_mixed_idx])), ğ’[cond_var_idx,1:end-T.nExo])

ForwardDiff.gradient(x -> sum(abs2, invjac * (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * x)), state[T.past_not_future_and_mixed_idx])

ForwardDiff.gradient(x -> sum(abs2, invjac * (data_in_deviations[:,i] - x)), ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])

ForwardDiff.gradient(x -> sum(abs2, invjac * x), data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])


# i = 2
# res = Optim.optimize(x -> minimize_distance_to_data(x, data_in_deviations[:,i], state, state_update, cond_var_idx, precision_factor, pruning), 
#                     zeros(T.nExo), 
#                     Optim.LBFGS(linesearch = LineSearches.BackTracking(order = 3)), 
#                     Optim.Options(f_abstol = eps(), g_tol= 1e-30); 
#                     autodiff = :forward)

#                     res.minimizer
# # data_in_deviations[:,i] - ğ’[cond_var_idx,end-T.nExo+1:end] * x

# @benchmark x = ğ’[cond_var_idx,end-T.nExo+1:end] \ data_in_deviations[:,i]
# @profview for k in 1:1000 ğ’[cond_var_idx,end-T.nExo+1:end] \ data_in_deviations[:,i] end


# @profview for k in 1:1000
@benchmark begin
    state = zeros(23)
    # statetmp = zeros(23)
    shocksÂ² = 0.0
    logabsdets = 0.0
    y = zeros(length(cond_var_idx))
    x = zeros(T.nExo)
    # state_reduced = zeros(T.nPast_not_future_and_mixed + T.nExo)

    jac = ğ’[cond_var_idx,end-T.nExo+1:end]

    if T.nExo == length(observables)
        logabsdets = â„’.logabsdet(-jac' ./ precision_factor)[1]
        jacdecomp = â„’.lu!(jac)
        invjac = inv(jacdecomp)
    else
        logabsdets = sum(x -> log(abs(x)), â„’.svdvals(-jac' ./ precision_factor))
        jacdecomp = â„’.svd!(jac)
        invjac = inv(jacdecomp)
    end

    logabsdets *= size(data_in_deviations,2) - presample_periods
    
    @views ğ’obs = ğ’[cond_var_idx,1:end-T.nExo]

    for i in axes(data_in_deviations,2)
        @views â„’.mul!(y, ğ’obs, state[T.past_not_future_and_mixed_idx])
        @views â„’.axpby!(1, data_in_deviations[:,i], -1, y)
        â„’.mul!(x,invjac,y)
        # x = invjac * (data_in_deviations[:,i] - ğ’[cond_var_idx,1:end-T.nExo] * state[T.past_not_future_and_mixed_idx])

        if i > presample_periods
            shocksÂ² += sum(abs2,x)
        end

        # # copyto!(state_reduced, 1, state, T.past_not_future_and_mixed_idx)
        # for (i,v) in enumerate(T.past_not_future_and_mixed_idx)
        #     state_reduced[i] = state[v]
        # end
        # copyto!(state_reduced, T.nPast_not_future_and_mixed + 1, x, 1, T.nExo)
        
        â„’.mul!(state, ğ’, vcat(state[T.past_not_future_and_mixed_idx], x))
        # state = state_update(state, x)
    end

    -(logabsdets + shocksÂ² + (length(observables) * (warmup_iterations + n_obs - presample_periods)) * log(2 * 3.141592653589793)) / 2
end

pruning = false

jac = ForwardDiff.jacobian( xx -> precision_factor .* abs.(data_in_deviations[:,i] - (pruning ? sum(state_update(state, xx)) : state_update(state, xx))[cond_var_idx]), x)'


res = precision_factor .* abs.(data_in_deviations[:,i] - (pruning ? sum(state_update(state, x)) : state_update(state, x))[cond_var_idx])


state = state_update(state, x)


x = Optim.minimizer(res)

res  = zeros(0)

jacc = zeros(T.nExo, length(observables))

match_data_sequence!(res, x, jacc, data_in_deviations[:,i], state, state_update, cond_var_idx, precision_factor)


match_data_sequence!(res, x, jacc, data_in_deviations[:,i], state, state_update, cond_var_idx, precision_factor)


@benchmark begin
shocksÂ² = 0.0
logabsdets = 0.0
state = zeros(23)

for i in axes(data_in_deviations,2)
    res = Optim.optimize(x -> minimize_distance_to_data(x, data_in_deviations[:,i], state, state_update, cond_var_idx, precision_factor, pruning), 
                    zeros(T.nExo), 
                    Optim.LBFGS(linesearch = LineSearches.BackTracking(order = 3)), 
                    Optim.Options(f_abstol = eps(), g_tol= 1e-30); 
                    autodiff = :forward)

    matched = Optim.minimum(res) < 1e-12

    if !matched # for robustness try other linesearch
        res = Optim.optimize(x -> minimize_distance_to_data(x, data_in_deviations[:,i], state, state_update, cond_var_idx, precision_factor, pruning), 
                        zeros(T.nExo), 
                        Optim.LBFGS(), 
                        Optim.Options(f_abstol = eps(), g_tol= 1e-30); 
                        autodiff = :forward)
    
        matched = Optim.minimum(res) < 1e-12
    end

    if !matched return -Inf end

    x = Optim.minimizer(res)

    res  = zeros(0)

    jacc = zeros(T.nExo, length(observables))

    match_data_sequence!(res, x, jacc, data_in_deviations[:,i], state, state_update, cond_var_idx, precision_factor)

    if i > presample_periods
        # due to change of variables: jacobian determinant adjustment
        if T.nExo == length(observables)
            logabsdets += â„’.logabsdet(jacc ./ precision_factor)[1]
        else
            logabsdets += sum(x -> log(abs(x)), â„’.svdvals(jacc ./ precision_factor))
        end

        shocksÂ² += sum(abs2,x)
    end

    state = state_update(state, x)
end

# See: https://pcubaborda.net/documents/CGIZ-final.pdf
 -(logabsdets + shocksÂ² + (length(observables) * (warmup_iterations + n_obs - presample_periods)) * log(2 * 3.141592653589793)) / 2
end